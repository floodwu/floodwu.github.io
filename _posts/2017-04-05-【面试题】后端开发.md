---
layout: post
title:  "面试题整理：后端开发"
date: 2017-04-05 00:00:01
categories: 面试题
tags: 后端开发
excerpt: "后端开发相关的面试题，包括Web服务器、网络协议、计算机系统、架构、数据库、缓存等等"
---

# RESTful架构风格理解

REST并不是一种具体的技术，也不是一种具体的规范，`REST其实是一种内涵非常丰富的架构风格`。它是为运行在互联网环境的分布式超媒体系统量身定制的。互联网环境与企业内网环境有非常大的差别，最主要的差别是两个方面：
（1）可伸缩性需求无法控制：并发访问量可能会暴涨，也可能会暴跌。
（2）安全性需求无法控制：无法控制客户端发来的请求的格式，很可能会是恶意的请求。

从架构风格的抽象高度来看，常见的分布式应用架构风格有三种：
（1）分布式对象（Distributed Objects，简称DO），架构实例有CORBA/RMI/EJB/DCOM/.NET Remoting等等；
（2）远程过程调用（Remote Procedure Call，简称RPC），架构实例有SOAP/XML-RPC/Hessian/Flash AMF/DWR等等；
（3）表述性状态转移（Representational State Transfer，简称REST），架构实例有HTTP/WebDAV；

REST是HTTP/1.1协议等Web规范的设计指导原则，`HTTP/1.1协议正是为实现REST风格的架构而设计的`。REST是所有Web应用都应该遵守的架构设计指导原则。当然，REST并不是法律，违反了REST的指导原则，仍然能够实现应用的功能。但是违反了REST的指导原则，会付出很多代价，特别是对于大流量的网站而言。
要深入理解REST，需要理解REST的五个关键词：
（1）资源（Resource）
资源是一种看待服务器的方式，即，`将服务器看作是由很多离散的资源组成`。每个资源是服务器上一个可命名的抽象概念。因为资源是一个抽象的概念，所以它不仅仅能代表服务器文件系统中的一个文件、数据库中的一张表等等具体的东西，可以将资源设计的要多抽象有多抽象，只要想象力允许而且客户端应用开发者能够理解。与面向对象设计类似，资源是以名词为核心来组织的，首先关注的是名词。一个资源可以由一个或多个URI来标识。URI既是资源的名称，也是资源在Web上的地址。对某个资源感兴趣的客户端应用，可以通过资源的URI与其进行交互。
（2）资源的表述（Representation）
资源的表述是一段对于资源在某个特定时刻的状态的描述。可以在客户端-服务器端之间转移（交换）。资源的表述可以有多种格式，例如HTML/XML/JSON/纯文本/图片/视频/音频等等。资源的表述格式可以通过协商机制来确定。请求-响应方向的表述通常使用不同的格式。
（3）状态转移（State Transfer）
状态转移（state transfer）与状态机中的状态迁移（state transition）的含义是不同的。状态转移说的是：在客户端和服务器端之间转移（transfer）代表资源状态的表述。通过转移和操作资源的表述，来间接实现操作资源的目的。
（4）统一接口（Uniform Interface）
REST要求，必须通过统一的接口来对资源执行各种操作。对于每个资源只能执行一组有限的操作。以HTTP/1.1协议为例，HTTP/1.1协议定义了一个操作资源的统一接口，主要包括以下内容：
1）7个HTTP方法：GET/POST/PUT/DELETE/PATCH/HEAD/OPTIONS
2）HTTP头信息（可自定义）
3）HTTP响应状态代码（可自定义）
4）一套标准的内容协商机制
5）一套标准的缓存机制
6）一套标准的客户端身份认证机制
（5）超文本驱动（Hypertext Driven）
“超文本驱动”又名“将超媒体作为应用状态的引擎”（Hypermedia As The Engine Of Application State，来自Fielding博士论文中的一句话，缩写为HATEOAS）。将Web应用看作是一个由很多状态（应用状态）组成的有限状态机。资源之间通过超链接相互关联，超链接既代表资源之间的关系，也代表可执行的状态迁移。在超媒体之中不仅仅包含数据，还包含了状态迁移的语义。以超媒体作为引擎，驱动Web应用的状态迁移。通过超媒体暴露出服务器所提供的资源，服务器提供了哪些资源是在运行时通过解析超媒体发现的，而不是事先定义的。从面向服务的角度看，超媒体定义了服务器所提供服务的协议。客户端应该依赖的是超媒体的状态迁移语义，而不应该对于是否存在某个URI或URI的某种特殊构造方式作出假设。一切都有可能变化，只有超媒体的状态迁移语义能够长期保持稳定。

REST风格的架构所具有的6个主要特征：
（1）面向资源（Resource Oriented）
（2）可寻址（Addressability）
（3）连通性（Connectedness）
（4）无状态（Statelessness）
（5）统一接口（Uniform Interface）
（6）超文本驱动（Hypertext Driven）
这6个特征是REST架构设计优秀程度的判断标准。其中，面向资源是REST最明显的特征，即，REST架构设计是以资源抽象为核心展开的。可寻址说的是：每一个资源在Web之上都有自己的地址。连通性说的是：应该尽量避免设计孤立的资源，除了设计资源本身，还需要设计资源之间的关联关系，并且通过超链接将资源关联起来。无状态、统一接口是REST的两种架构约束，超文本驱动是REST的一个关键词。


# TCP粘包
TCP是流式传送的也就是连接建立后可以一直不停的发送,并没有明确的边界定义。而用UDP发送的时候是可以按照一个一个数据包去发送的一个数据包就是一个明确的边界。因为TCP是流式传送，所以会开辟一个缓冲区，发送端往其中写入数据，每过一段时间就发送出去，因此有可能后续发送的数据（属于另一个包）和之前发送的数据同时存在缓冲区中并一起发送，早晨粘包。接收端也有缓存，因此也存在粘包。
处理粘包的唯一方法就是制定应用层的数据通讯协议，通过协议来规范现有接收的数据是否满足消息数据的需要。在应用中处理粘包的基础方法主要有两种分别是以4节字描述消息大小或以结束符，实际上也有两者相结合的如HTTP,redis的通讯协议等。

# TCP的Nagle算法

事实上，Nagle算法所谓的“提高网络利用率”只是它的一个副作用，`Nagle算法的主旨在于“避免发送‘大量’的小包”`。Nagle算法并没有阻止发送小包，它只是阻止了发送大量的小包！
TCP/IP协议中，无论发送多少数据，总是要在数据前面加上协议头，同时，对方接收到数据，也需要发送ACK表示确认。为了尽可能的利用网络带宽，TCP总是希望尽可能的发送足够大的数据。Nagle算法就是为了尽可能发送大块数据，避免网络中充斥着许多小数据块。
`Nagle算法的基本定义是任意时刻，最多只能有一个未被确认的小段`。 所谓“小段”，指的是小于MSS尺寸的数据块，所谓“未被确认”，是指一个数据块发送出去后，没有收到对方发送的ACK确认该数据已收到。Nagle的算法通常会在TCP程序里添加两行代码，在未确认数据发送的时候让发送器把数据送到缓存里。任何数据随后继续直到得到明显的数据确认或者直到攒到了一定数量的数据了再发包。
默认情况下，发送数据采用Nagle 算法。这样虽然提高了网络吞吐量，但是实时性却降低了，在一些交互性很强的应用程序来说是不允许的，使用TCP_NODELAY选项可以禁止Nagle 算法。

# TCP同时打开,同时关闭
两个应用程序同时执行主动打开的情况是可能的，虽然发生的可能性较低。每一端都发送一个SYN,并传递给对方，且每一端都使用对端所知的端口作为本地端口。例如：
主机a中一应用程序使用7777作为本地端口，并连接到主机b 8888端口做主动打开。
主机b中一应用程序使用8888作为本地端口，并连接到主机a 7777端口做主动打开。
`tcp协议在遇到这种情况时，只会打开一条连接`。
这个连接的建立过程需要4次数据交换，而一个典型的连接建立只需要3次交换（即3次握手）
但多数伯克利版的tcp/ip实现并不支持同时打开。
![image](http://woojean.com/images/net_10.png)

如果应用程序同时发送FIN，则在发送后会首先进入FIN_WAIT_1状态。在收到对端的FIN后，回复一个ACK，会进入CLOSING状态。在收到对端的ACK后，进入TIME_WAIT状态。这种情况称为同时关闭。
同时关闭也需要有4次报文交换，与典型的关闭相同。
![image](http://woojean.com/images/net_11.png)


# Referer头的安全问题

Referer是HTTP协议中的一个请求报头，`用于告知服务器用户的来源页面`。比如说从Google搜索结果中点击进入了某个页面，那么该次HTTP请求中的Referer就是Google搜索结果页面的地址。如果某篇博客中引用了其他地方的一张图片，那么对该图片的HTTP请求中的Referer就是你那篇博客的地址。
一般Referer主要用于统计，像CNZZ、百度统计等可以通过Referer统计访问流量的来源和搜索的关键词（包含在URL中）等等，方便站长们有针性对的进行推广和SEO。

Referer另一个用处就是`防盗链`。可以用referrer-killer（一个js库）来实现反反盗链。

Referer是由浏览器自动加上的，`以下情况是不带Referer的`
（1）直接输入网址或通过浏览器书签访问
（2）使用JavaScript的Location.href或者是Location.replace()
（3）HTTPS等加密协议

Referer的安全问题：以新浪微博曾经的一个漏洞（新浪微博`gsid劫持`）为例
gsid是一些网站移动版的认证方式，移动互联网之前较老的手机浏览器不支持cookie，为了能够识别用户身份（实现类似cookie的作用），就在用户的请求中加入了一个类似“sessionid”的字符串，通过GET方式传递，带有这个id的请求，就代表你的帐号发起的操作。后来又因用户多次认证体验不好，gsid的失效期是很长甚至永久有效的（即使改了密码也无用哦，这个问题在很多成熟的web产品上仍在发生）。也就是说，一旦攻击者获取到了这个gsid，就等同于长期拥有了你的身份权限，对你的帐号做任意操作。
gsid这个非常重要的参数竟然就在URL里，只要攻击者在微博上给你发一个链接（指向攻击者的服务器），你通过手机点击进入之后，手机当前页面的URL就通过Referer主动送到了攻击者的服务器上，攻击者自然就可以轻松拿到你的gsid进而控制你的账号。

# Redis通信协议实例
发送格式：
*<参数的个数>CR LF
$<参数1字节数>CR LF
<参数1>CR LF
...
$<参数n字节数>CR LF
<参数n>CR LF

例如`set mykey myvalue`命令，相应的字符串为：*3\r\n$3\r\nSET\r\n$5\r\nmykey\r\n$7\r\nmyvalue\r\n

响应格式：
响应的类型都是由返回数据的第一个字节决定的，有如下几种类型：
"+" 代表一个状态信息 如 +ok 
"-" 代表发送了错误  （如：操作运算操作了错误的类型）
":" 返回的是一个整数  格式如：":11\r\n。
 一些命令返回一些没有任何意义的整数，如LastSave返回一个时间戳的整数值， INCR返回一个加1后的数值；一些命令如exists将返回0或者1代表是否true or false；其他一些命令如SADD, SREM 在确实执行了操作时返回1 ，否则返回0
"$" 返回一个块数据，被用来返回一个二进制安全的字符串
"*" 返回多个块数据（用来返回多个值， 总是第一个字节为"*"， 后面写着包含多少个相应值，如：
C:LRANGE mylist 0 3
S:*4
S:$3
S:foo
S:$3
S:bar
S:$5
$:world
如果指定的值不存在，那么返回*0

# Redis的管道技术

Redis是一种基于客户端-服务端模型以及请求/响应协议的TCP服务。这意味着通常情况下一个请求会遵循以下步骤：
（1）客户端向服务端发送一个查询请求，并监听Socket返回，通常是以阻塞模式，等待服务端响应。
（2）服务端处理命令，并将结果返回给客户端。
Redis管道技术可以在服务端未响应时，客户端可以继续向服务端发送请求，并最终一次性读取所有服务端的响应。
如：
$(echo -en "PING\r\n SET w3ckey redis\r\nGET w3ckey\r\nINCR visitor\r\nINCR visitor\r\nINCR visitor\r\n"; sleep 10) | nc localhost 6379

+PONG
+OK
redis
:1
:2
:3
以上实例中通过使用PING命令查看redis服务是否可用，之后设置了w3ckey的值为redis，然后获取w3ckey的值并使得visitor自增3次。在返回的结果中可以看到`这些命令一次性向redis服务提交，并最终一次性读取所有服务端的响应`
管道技术最显著的优势是提高了redis服务的性能。

# 乐观锁与悲观锁的区别

`悲观锁`(Pessimistic Lock)每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。
`乐观锁`(Optimistic Lock)每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是`在更新的时候`会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。`乐观锁适用于多读的应用类型`，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。
两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁就比较合适。

# 僵尸状态是每个子进程必经的状态吗？

是的。 任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。这是每个子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。 * 如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。

# 分页和分段的主要区别

(1)页是信息的物理单位，分页是为实现离散分配方式，以消减内存的外零头，提高内存的利用率。或者说，分页仅仅是由于系统管理的需要而不是用户的需要。段则是信息的逻辑单位，它含有一组其意义相对完整的信息。分段的目的是为了能更好地满足用户的需要。 
(2)页的大小固定且由系统决定，由系统把逻辑地址划分为页号和页内地址两部分，是由机器硬件实现的，因而在系统中只能有一种大小的页面；而段的长度却不固定，决定于用户所编写的程序，通常由编译程序在对源程序进行编译时，根据信息的性质来划分。
(3)分页的作业地址空间是一维的，即单一的线性地址空间，程序员只需利用一个记忆符，即可表示一个地址；而分段的作业地址空间则是二维的，程序员在标识一个地址时，既需给出段名， 又需给出段内地址。

# 后台进程与守护进程有什么区别？

（1）最直观的区别：守护进程没有控制终端，而后台进程还有。如通过命令firefox &在后台运行firefox，此时firefox虽然在后台运行，但是并没有脱离终端的控制，如果把终端关掉则firefox也会一起关闭。
（2）后台进程的文件描述符继承自父进程，例如shell，所以它也可以在当前终端下显示输出数据。但是`守护进程自己变成进程组长`，其文件描述符号和控制终端没有关联，是控制台无关的。
（3）`守护进程肯定是后台进程，但后台进程不一定是守护进程`。基本上任何一个程序都可以后台运行，但守护进程是具有特殊要求的程序，比如它能够脱离自己的父进程，成为自己的会话组长等（这些需要在程序代码中显式地写出来）。

# 如何查看僵尸进程？

$ ps -el 其中，有标记为Z的进程就是僵尸进程 S代表休眠状态；D代表不可中断的休眠状态；R代表运行状态；Z代表僵死状态；T代表停止或跟踪状态
僵尸进程变为孤儿进程：父进程死后，僵尸进程成为"孤儿进程"，过继给1号进程init，init始终会负责清理僵尸进程．它产生的所有僵尸进程也跟着消失。

# 如何查看Linux进程之间的关系？

ps -o pid,pgid,ppid,comm | cat

输出：
  PID  PGID  PPID COMMAND
 3003  3003  2986 su
 3004  3004  3003 bash
 3423  3423  3004 ps
 3424  3423  3004 cat

每个进程都会属于一个进程组(process group)，每个进程组中可以包含多个进程。进程组会有一个进程组领导进程 (process group leader)，领导进程的PID (PID见Linux进程基础)成为进程组的ID (process group ID, PGID)，以识别进程组。PID为进程自身的ID，PGID为进程所在的进程组的ID， PPID为进程的父进程ID。

# 子进程结束后为什么要进入僵尸状态? 

因为父进程可能要取得子进程的退出状态等信息。 

# 寄存器及其作用

数据寄存器 - 用来储存整数数字。
浮点寄存器- 用来储存浮点数字。
地址寄存器 - 持有存储器地址，以及用来访问存储器。
通用目的寄存器  - 可以保存数据或地址两者，也就是说他们是结合 数据/地址 寄存器的功用。
常数寄存器 - 用来持有只读的数值（例如 0、1、圆周率等等）。
向量寄存器 - 用来储存由向量处理器运行SIMD（Single Instruction, Multiple Data）指令所得到的数据。
指令寄存器（instruction register） - 储存现在正在被运行的指令
索引寄存器（index register） - 是在程序运行实用来更改运算对象地址之用。
特殊目的寄存器 - 储存CPU内部的数据，像是程序计数器（或称为指令指针），堆栈寄存器，以及状态寄存器（或称微处理器状态字组）。

# 死锁产生的四个必要条件

（1）互斥条件：一个资源每次只能被一个进程使用。
（2）请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
（3）不剥夺条件:进程已获得的资源，在未使用完之前，不能强行剥夺。
（4）循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。
这四个条件是死锁的必要条件。只要系统发生死锁，这些条件必然成立，而只要上述条件之
一不满足，就不会发生死锁。

#  用户身份切换

将一般用户变成root：

```
[test@test test]$ su
```

将身份变成username的身份:

```
[root @test /root ]# sudo [-u username] [command]
[test@test test]$ sudo mkdir /root/testing
```

root可以执行test用户的指令，建立test的文件不需要root的密码仍可以执行root的工具，这时就可以使用sudo。由于执行root身份的工作时，输入的密码是用户的密码，而不是root的密码，所以可以减少root密码外泄的可能性。

# 简述ext2和ext3的区别？

Linux ext2/ext3文件系统使用索引节点来记录文件信息，作用像windows的文件分配表。索引节点是一个结构，它包含了一个文件的长度、创建及修改时间、权限、所属关系、磁盘中的位置等信息。一个文件系统维护了一个索引节点的数组，每个文件或目录都与索引节点数组中的唯一一个元素对应。系统给每个索引节点分配了一个号码，也就是该节点在数组中的索引号，称为索引节点号。 linux文件系统将文件索引节点号和文件名同时保存在目录中。所以，目录只是将文件的名称和它的索引节点号结合在一起的一张表，目录中每一对文件名称和索引节点号称为一个连接。 对于一个文件来说有唯一的索引节点号与之对应，对于一个索引节点号，却可以有多个文件名与之对应。因此，在磁盘上的同一个文件可以通过不同的路径去访问它。
Linux缺省情况下使用的文件系统为Ext2，ext2文件系统的确高效稳定。但是，随着Linux系统在关键业务中的应用，Linux文件系统的弱点也渐渐显露出来了:其中系统缺省使用的ext2文件系统是非日志文件系统。这在关键行业的应用是一个致命的弱点。本文向各位介绍Linux下使用ext3日志文件系统应用。
Ext3文件系统是直接从Ext2文件系统发展而来，目前ext3文件系统已经非常稳定可靠。它完全兼容ext2文件系统。用户可以平滑地过渡到一个日志功能健全的文件系统中来。这实际上了也是ext3日志文件系统初始设计的初衷。
Ext3日志文件系统的特点
1、高可用性
系统使用了ext3文件系统后，即使在非正常关机后，系统也不需要检查文件系统。宕机发生后，恢复ext3文件系统的时间只要数十秒钟。
2、数据的完整性:
ext3文件系统能够极大地提高文件系统的完整性，避免了意外宕机对文件系统的破坏。在保证数据完整性方面，ext3文件系统有2种模式可供选择。其中之一就是“同时保持文件系统及数据的一致性”模式。采用这种方式，你永远不再会看到由于非正常关机而存储在磁盘上的垃圾文件。
3、文件系统的速度:
尽管使用ext3文件系统时，有时在存储数据时可能要多次写数据，但是，从总体上看来，ext3比ext2的性能还要好一些。这是因为ext3的日志功能对磁盘的驱动器读写头进行了优化。所以，文件系统的读写性能较之Ext2文件系统并来说，性能并没有降低。
4、数据转换
由ext2文件系统转换成ext3文件系统非常容易，只要简单地键入两条命令即可完成整个转换过程，用户不用花时间备份、恢复、格式化分区等。用一个ext3文件系统提供的小工具tune2fs，它可以将ext2文件系统轻松转换为ext3日志文件系统。另外，ext3文件系统可以不经任何更改，而直接加载成为ext2文件系统。
5、多种日志模式
Ext3有多种日志模式，一种工作模式是对所有的文件数据及metadata（定义文件系统中数据的数据,即数据的数据）进行日志记录（data=journal模式）；另一种工作模式则是只对metadata记录日志，而不对数据进行日志记录，也即所谓data=ordered或者data=writeback模式。系统管理人员可以根据系统的实际工作要求，在系统的工作速度与文件数据的一致性之间作出选择。

# 进程为什么要挂起？

在多进程程序系统中，进程在处理器上交替运行，在运行、就绪和阻塞3种基本状态之间不断地发生变化。由于进程的不断创建，系统资源（特别是主存资源）已不能满足进程运行的要求。此时就必须将某些进程挂起，对换到磁盘镜像区，暂时不参与进程调度，以平衡系统负载的目的。如果系统出现故障，或者是用户调试程序，也可能需要将进程挂起检查问题。        
所谓挂起状态，实际上就是一种静止的状态。一个进程被挂起后，不管它是否在就绪状态，系统都不分配给它处理机。（区别于阻塞状态）。这样进程的三态模型（执行、就绪、阻塞）就变为五态模型：执行状态、活动就绪状态、静止就绪状态、活动阻塞状态和静止阻塞状态 
活动就绪：指进程在主存并且可被调度的状态 （对应于三态的就绪状态）
静止就绪：指进程被对换到辅存时的就绪状态，是不能被直接调度的状态，只有当主存中没有活动就绪态进程，或者是挂起态进程具有更高的优先级，系统将把挂起就绪态进程调回主存并转换为活动就绪。 
活动阻塞：指进程在主存中。一旦等待的事件产生，便进入活动就绪状态（对应于三态的阻塞状态） 
静止阻塞：指进程对换到辅存时的阻塞状态。一旦等待的事件产生，便进入静止就绪状态。

# 进程通信的类型 

1. 共享存储器系统(Shared-Memory System)（全局变量）
   (1)基于共享数据结构的通信方式。 
   (2)基于共享存储区的通信方式。 
2. 消息传递系统(Message passing system)
   进程间的数据交换，是以格式化的消息(message)为单位的；在计算机网络中，又把message称为报文。
3. 管道(Pipe)通信
   “管道”，是指用于连接一个读进程和一个写进程以实现他们之间通信的一个共享文件，又名pipe文件。向管道(共享文件)提供输入的发送进程(即写进程)， 以字符流形式将大量的数据送入管道；而接受管道输出的接收进程(即读进程)，则从管道中接收(读)数据。
   为了协调双方的通信，管道机制必须提供以下三方面的协调能力：
   ① 互斥，即当一个进程正在对pipe执行读/写操作时，其它(另一)进程必须等待。 
   ② 同步，指当写(输入)进程把一定数量(如4 KB)的数据写入pipe，便去睡眠等待， 直到读(输出)进程取走数据后，再把他唤醒。当读进程读一空pipe时，也应睡眠等待，直至写进程将数据写入管道后，才将之唤醒。
   ③ 确定对方是否存在，只有确定了对方已存在时，才能进行通信。

# LINUX中什么是僵尸进程和孤儿进程，它们是否消耗系统资源？

僵尸进程将会导致资源浪费，而孤儿则不会。
由于子进程的结束和父进程的运行是一个异步过程,即父进程永远无法预测子进程到底什么时候结束. 那么会不会因为父进程太忙来不及wait子进程,或者说不知道子进程什么时候结束,而丢失子进程结束时的状态信息呢? 不会。因为UNIX提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息，就可以得到。这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息(包括进程号、退出状态、运行时间等)。直到父进程通过wait / waitpid来取时才释放. 但这样就导致了问题,如果进程不调用wait/waitpid的话, 那么保留的那段信息就不会释放,其进程号就会一直被占用,但是系统所能使用的进程号是有限的,如果大量的产生僵死进程,将因为没有可用的进程号而导致系统不能产生新的进程. 此即为僵尸进程的危害,应当避免。

# Linux中通过编译安装的方式安装程序，各步骤操作分别做什么工作？

源码要运行，必须先转成二进制的机器码。这是编译器的任务。
对于简单的代码，可以直接调用编译器生成二进制文件后运行，如：

```
$ gcc test.c
$ ./a.out
```
对于复杂的项目，编译过程通常分成3个部分：
```
$ ./configure
$ make  
$ make install
```

整个编译安装过程分为以下步骤：
（1）配置
配置信息保存在一个配置文件之中，约定俗成是一个叫做configure的脚本文件。通常它是由autoconf工具生成的。编译器通过运行这个脚本，获知编译参数。如果用户的系统环境比较特别，或者有一些特定的需求，就需要手动向configure脚本提供编译参数，如：
$ ./configure --prefix=/www --with-mysql  # 指定安装后的文件保存在www目录，并且编译时加入mysql模块的支持

（2）确定标准库和头文件的位置
从配置文件中知道标准库和头文件的位置。

（3）确定依赖关系
源码文件之间往往存在依赖关系，编译器需要确定编译的先后顺序。假定A文件依赖于B文件，编译器应该保证做到下面两点。
1）只有在B文件编译完成后，才开始编译A文件。
2）当B文件发生变化时，A文件会被重新编译。
编译顺序保存在一个叫做makefile的文件中，里面列出哪个文件先编译，哪个文件后编译。而makefile文件由configure脚本运行生成，这就是为什么编译时configure必须首先运行的原因。

（4）预编译头文件
不同的源码文件，可能引用同一个头文件（比如stdio.h）。编译的时候，头文件也必须一起编译。为了节省时间，编译器会在编译源码之前，先编译头文件。这保证了头文件只需编译一次，不必每次用到的时候，都重新编译了。不过，并不是头文件的所有内容都会被预编译。用来声明宏的#define命令，就不会被预编译。

（5）预处理
编译器就开始替换掉源码中的头文件和宏以及移除注释。

（6）编译
编译器就开始生成机器码。对于某些编译器来说，还存在一个中间步骤，会先把源码转为汇编码（assembly），然后再把汇编码转为机器码。这种转码后的文件称为对象文件（object file）。

（7）链接
把外部函数的代码（通常是后缀名为.lib和.a的文件）添加到可执行文件中。这就叫做连接（linking）。这种通过拷贝，将外部函数库添加到可执行文件的方式，叫做静态连接（static linking）
make命令的作用，就是从第（4）步头文件预编译开始，一直到做完这一步。

（8）安装
将可执行文件保存到用户事先指定的安装目录。这一步还必须完成创建目录、保存文件、设置权限等步骤。这整个的保存过程就称为"安装"（Installation）。

（9）操作系统链接
以某种方式通知操作系统，让其知道可以使用这个程序了。这就要求在操作系统中，登记这个程序的元数据：文件名、文件描述、关联后缀名等等。Linux系统中，这些信息通常保存在/usr/share/applications目录下的.desktop文件中。
make install命令，就用来完成"安装"和"操作系统连接"这两步。

（10）生成安装包
将上一步生成的可执行文件，做成可以分发的安装包。通常是将可执行文件（连带相关的数据文件），以某种目录结构，保存成压缩文件包，交给用户。

（11）动态链接
开发者可以在编译阶段选择可执行文件连接外部函数库的方式，到底是静态连接（编译时连接），还是动态连接（运行时连接）。
静态连接就是把外部函数库，拷贝到可执行文件中。这样做的好处是，适用范围比较广，不用担心用户机器缺少某个库文件；缺点是安装包会比较大，而且多个应用程序之间，无法共享库文件。
动态连接的做法正好相反，外部函数库不进入安装包，只在运行时动态引用。好处是安装包会比较小，多个应用程序可以共享库文件；缺点是用户必须事先安装好库文件，而且版本和安装位置都必须符合要求，否则就不能正常运行。
现实中，大部分软件采用动态连接，共享库文件。这种动态共享的库文件，Linux平台是后缀名为.so的文件，Windows平台是.dll文件，Mac平台是.dylib文件。

# Linux程序、进程管理

Linux操作系统包括如下3种不同类型的进程，每种进程都有其自己的特点和属性。
（1）交互进程：由shell启动的进程。可在前台运行，也可在后台运行；
（2）批处理进程：一个进程序列；
（3）守护进程：守护进程（Daemon，也称为精灵进程）是指在后台运行而又没有启动终端或登录shell。守护进程一般由系统开机时通过脚本（script）自动激活启动或者由root用户通过shell启动。守护进程总是活跃的，一般在后台运行，所以它所处的状态是等待处理任务的请求。
启动守护进程有如下几种方法：
1）在引导系统时启动：通过脚本启动，这些脚本一般位于/etc/rc.d中。在/etc目录下的很多rc文件都是启动脚本 。rc0.d，rc1.d,rc2.d,rc3.d,rc4.d,rc5.d,rc6.d,其中的数字代表在指定的runlevel下运行相应的描述，0代表关机，6代表重启。其中，以k开头的文件表示关闭，以s开头的文件表示重启。可查看相应文件夹中的readme文件。rc0.d，rc1.d,rc2.d,rc3.d,rc4.d,rc5.d,rc6.d，rcS.d都连接到/etc/init.d文件夹，该目录中存放着守护进程的运行文件。
2）人工手动从shell提示符启动：任何具有权限的用户都可以启动相应的守护进程
root@Ubuntu:~# /etc/init.d/vsftpd start//启动ＦＴＰ服务器，ubuntu下默认已经安装了vsfptd服务器
3）使用crond守护进程启动
4）执行at命令启动
后台进程：在shell下直接输入命令后，shell将进程放到前台执行。如果要将进程放到后台执行，需要在命令行的结尾加上一个 “ & ” 符号。例如： root@Ubuntu:~#  find / -name passwd&
一般将一些比较耗时的操作放到后台执行。
前台进程与后台进程的区别：前台进程有控制终端，后台进程没有控制终端,所以没有结果可以显示。 前台进程绝大部分是用户进程，后台的一般（大多数是）系统进程。守护进程都是后台进程
进程的执行模式划分为用户模式和内核模式：系统进程，只运行在内核模式，执行操作系统代码，完成一些管理性的工作。用户进程，运行在用户模式下，通过系统调用或在出现中断、异常时进入内核模式。

# nohup与&的区别

& 要是关闭终端那么脚本也停了，
加nohup  既使把终端关了，脚本也会跑，是在服务器那运行的。

nohup 命令运行由 Command 参数和任何相关的 Arg 参数指定的命令，忽略所有挂断（SIGHUP）信号。在注销后使用 nohup 命令运行后台中的程序。

一般结合使用：nohup command & 

# select、poll、epoll

文件描述符（fd）：文件描述符是一个简单的整数，用以标明每一个被进程所打开的文件和socket的索引。第一个打开的文件是0，第二个是1，依此类推。最前面的三个文件描述符（0，1，2）分别与标准输入（stdin），标准输出（stdout）和标准错误（stderr）对应。`Unix 操作系统通常给每个进程能打开的文件数量强加一个限制。当用完所有的文件描述符后，将不能接收用户新的连接，直到一部分当前请求完成，相应的文件和socket被关闭`。

select，poll，epoll都是IO多路复用的机制。`I/O多路复用通过一种机制，可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作`。select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。

epoll的改进：
（1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。`虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了`，这节省了大量的CPU时间。这就是回调机制带来的性能提升（本质的改进在于epoll采用基于事件的就绪通知方式）。
（2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。（另一个本质的改进就是使用了内存映射（mmap）技术）

epoll被公认为Linux2.6下性能最好的多路I/O就绪通知方法，实现高效处理百万句柄。

# SOCKET函数

socket()
我们使用系统调用socket()来获得文件描述符：
int socket(int domain,int type,int protocol);
第一个参数domain设置为“AF_INET”。
第二个参数是套接口的类型：SOCK_STREAM或SOCK_DGRAM。
第三个参数设置为0。
系统调用socket()只返回一个套接口描述符，如果出错，则返回-1。 

bind()
一旦你有了一个套接口以后，下一步就是把套接口绑定到本地计算机的某一个端口上。但如果你只想使用connect()则无此必要。下面是系统调用bind()的使用方法：
int bind(int sockfd,struct sockaddr*my_addr,int addrlen);
第一个参数sockfd是由socket()调用返回的套接口文件描述符。
第二个参数my_addr是指向数据结构sockaddr的指针。数据结构sockaddr中包括了关于你的地址、端口和IP地址的信息。
第三个参数addrlen可以设置成sizeof(structsockaddr)。
下面是一个例子：
```c
#define MYPORT 3490
main()
{
  int sockfd;
  struct sockaddr_in my_addr;
  sockfd=socket(AF_INET,SOCK_STREAM,0); /*do someerror checking!*/
  my_addr.sin_family=AF_INET; /*hostbyteorder*/
  my_addr.sin_port=htons(MYPORT); /*short,network byte order*/
  my_addr.sin_addr.s_addr=inet_addr('132.241.5.10');
  bzero(&(my_addr.sin_zero),8); /*zero the rest of the struct*/
  /*don't forget your error checking for bind():*/
  bind(sockfd,(struct sockaddr*)&my_addr,sizeof(struct sockaddr));
  ...
```
  如果出错，bind()也返回-1。
  如果你使用connect()系统调用，那么你不必知道你使用的端口号。当你调用connect()时，它检查套接口是否已经绑定，如果没有，它将会分配一个空闲的端口。 

connect()
系统调用connect()的用法如下：
int connect(int sockfd,struct sockaddr* serv_addr,int addrlen);
第一个参数还是套接口文件描述符，它是由系统调用socket()返回的。
第二个参数是serv_addr是指向数据结构sockaddr的指针，其中包括目的端口和IP地址。
第三个参数可以使用sizeof(structsockaddr)而获得。
下面是一个例子：
```c
#define DEST_IP '132.241.5.10'
#define DEST_PORT 23
main()
{
  intsockfd;
  struct sockaddr_in dest_addr; /*will hold the destination addr*/
  sockfd=socket(AF_INET,SOCK_STREAM,0); /*do some error checking!*/
  dest_addr.sin_family=AF_INET; /*hostbyteorder*/
  dest_addr.sin_port=htons(DEST_PORT);/*short,network byte order*/
  dest_addr.sin_addr.s_addr=inet_addr(DEST_IP);
  bzero(&(dest_addr.sin_zero),8);/*zero the rest of the struct*/
    /*don'tforgettoerrorchecktheconnect()!*/
  connect(sockfd,(structsockaddr*)&dest_addr,sizeof(struct sockaddr));
  ...
  同样，如果出错，connect()将会返回-1。 
  
listen()
```
如果你希望不连接到远程的主机，也就是说你希望等待一个进入的连接请求，然后再处理它们。这样，你通过首先调用listen()，然后再调用accept()来实现。
系统调用listen()的形式如下：
int listen(int sockfd,int backlog);
第一个参数是系统调用socket()返回的套接口文件描述符。
第二个参数是进入队列中允许的连接的个数。进入的连接请求在使用系统调用accept()应答之前要在进入队列中等待。这个值是队列中最多可以拥有的请求的个数。大多数系统的缺省设置为20。你可以设置为5或者10。当出错时，listen()将会返回-1值。
当然，在使用系统调用listen()之前，我们需要调用bind()绑定到需要的端口，否则系统内核将会让我们监听一个随机的端口。
所以，如果你希望监听一个端口，下面是应该使用的系统调用的顺序：
  socket();
  bind();
  listen();
    /*accept()goes here*/ 

accept()
系统调用accept()比较起来有点复杂。在远程的主机可能试图使用connect()连接你使用listen()正在监听的端口。但此连接将会在队列中等待，直到使用accept()处理它。调用accept()之后，将会返回一个全新的套接口文件描述符来处理这个单个的连接。这样，对于同一个连接来说，你就有了两个文件描述符。原先的一个文件描述符正在监听你指定的端口，新的文件描述符可以用来调用send()和recv()。
调用的例子如下：
int accept(intsockfd,void*addr,int*addrlen);
第一个参数是正在监听端口的套接口文件描述符。
第二个参数addr是指向本地的数据结构
sockaddr_in的指针。调用connect()中的信息将存储在这里。通过它你可以了解哪个主机在哪个端口呼叫你。
第三个参数同样可以使用sizeof(structsockaddr_in)来获得。
如果出错，accept()也将返回-1。下面是一个简单的例子：
```c
#define MYPORT 3490 /*theportuserswillbeconnectingto*/
#define BACKLOG 10/*howmanypendingconnectionsqueuewillhold*/
main()
{
  intsockfd,new_fd;/*listenonsock_fd,newconnectiononnew_fd*/
  struct sockaddr_in my_addr;/*myaddressinformation*/
  struct sockaddr_in their_addr;/*connector'saddressinformation*/
  int sin_size;
  sockfd=socket(AF_INET,SOCK_STREAM,0); /*dosomeerrorchecking!*/
  my_addr.sin_family=AF_INET; /*hostbyteorder*/
  my_addr.sin_port=htons(MYPORT); /*short,networkbyteorder*/
  my_addr.sin_addr.s_addr=INADDR_ANY;/*auto-fillwithmyIP*/
  bzero(&(my_addr.sin_zero),8);/*zerotherestofthestruct*/
    /*don'tforgetyourerrorcheckingforthesecalls:*/
  bind(sockfd,(structsockaddr*)&my_addr,sizeof(structsockaddr));
  listen(sockfd,BACKLOG);
  sin_size=sizeof(structsockaddr_in);
  new_fd=accept(sockfd,&their_addr,&sin_size);
  ...
```
下面，我们将可以使用新创建的套接口文件描述符new_fd来调用send()和recv()。 

send() 和recv()
系统调用send()的用法如下：
int send(int sockfd,const void* msg,int len,int flags);
第一个参数是你希望给发送数据的套接口文件描述符。它可以是你通过socket()系统调用返回的，也可以是通过accept()系统调用得到的。
第二个参数是指向你希望发送的数据的指针。
第三个参数是数据的字节长度。第四个参数标志设置为0。
下面是一个简单的例子：
```c
char* msg='Beejwashere!';
int len,bytes_sent;
..
len=strlen(msg);
bytes_sent=send(sockfd,msg,len,0);
...
```
系统调用send()返回实际发送的字节数，这可能比你实际想要发送的字节数少。如果返回的字节数比要发送的字节数少，你在以后必须发送剩下的数据。当send()出错时，将返回-1。
系统调用recv()的使用方法和send()类似：
int recv(int sockfd,void* buf,int len,unsigned int flags);
第一个参数是要读取的套接口文件描述符。
第二个参数是保存读入信息的地址。
第三个参数是缓冲区的最大长度。第四个参数设置为0。
系统调用recv()返回实际读取到缓冲区的字节数，如果出错则返回-1。
这样使用上面的系统调用，你可以通过数据流套接口来发送和接受信息。 

sendto() 和recvfrom()
因为数据报套接口并不连接到远程的主机上，所以在发送数据包之前，我们必须首先给出目的地址，请看：
int sendto(int sockfd,const void* msg,int len,unsigned int flags,
                conststruct sockaddr*to,inttolen);
除了两个参数以外，其他的参数和系统调用send()时相同。
参数to是指向包含目的IP地址和端口号的数据结构sockaddr的指针。
参数tolen可以设置为sizeof(structsockaddr)。
系统调用sendto()返回实际发送的字节数，如果出错则返回-1。
系统调用recvfrom()的使用方法也和recv()的十分近似：
int recvfrom(int sockfd,void* buf,int len,unsigned int flags
            struct sockaddr* from,int* fromlen);
参数from是指向本地计算机中包含源IP地址和端口号的数据结构sockaddr的指针。
参数fromlen设置为sizeof(struct sockaddr)。
系统调用recvfrom()返回接收到的字节数，如果出错则返回-1。 

close() 和shutdown()
你可以使用close()调用关闭连接的套接口文件描述符：
close(sockfd);
这样就不能再对此套接口做任何的读写操作了。
使用系统调用shutdown()，可有更多的控制权。它允许你在某一个方向切断通信，或者切断双方的通信：
int shutdown(int sockfd,int how);
第一个参数是你希望切断通信的套接口文件描述符。第二个参数how值如下：
0—Furtherreceivesaredisallowed
1—Furthersendsaredisallowed
2—Furthersendsandreceivesaredisallowed(likeclose())
shutdown()如果成功则返回0，如果失败则返回-1。 

getpeername()
这个系统的调用十分简单。它将告诉你是谁在连接的另一端：
int getpeername(int sockfd,struct sockaddr* addr,int* addrlen);
第一个参数是连接的数据流套接口文件描述符。
第二个参数是指向包含另一端的信息的数据结构sockaddr的指针。
第三个参数可以设置为sizeof(structsockaddr)。
如果出错，系统调用将返回-1。
一旦你获得了它们的地址，你可以使用inet_ntoa()或者gethostbyaddr()来得到更多的信息。

gethostname()
系统调用gethostname()比系统调用getpeername()还简单。它返回程序正在运行的计算机的名字。系统调用gethostbyname()可以使用这个名字来决定你的机器的IP地址。
下面是一个例子：
int gethostname(char*hostname,size_tsize);
如果成功，gethostname将返回0。如果失败，它将返回-1。 
•htonl()：把32位值从主机字节序转换成网络字节序 
•htons()：把16位值从主机字节序转换成网络字节序 
•ntohl()：把32位值从网络字节序转换成主机字节序 
•ntohs()：把16位值从网络字节序转换成主机字节序 

设置Socket缓冲区 
int zero = 0;
setsockopt( ov->m_Socket, SOL_SOCKET, SO_SNDBUF, (char *) &zero, sizeof zero );
setsockopt( ov->m_Socket, SOL_SOCKET, SO_RCVBUF, (char *) &zero, sizeof zero );






# MySQL语句的执行顺序
MySQL的语句一共分为11步，最先执行的总是FROM操作，最后执行的是LIMIT操作。`其中每一个操作都会产生一张虚拟的表`，这个虚拟的表作为一个处理的输入，只是这些虚拟的表对用户来说是透明的，但是只有最后一个虚拟的表才会被作为结果返回。如果没有在语句中指定某一个子句，那么将会跳过相应的步骤。
1.`FORM:` 对FROM的左边的表和右边的表计算笛卡尔积。产生虚表VT1
2.`ON:` 对虚表VT1进行ON筛选，只有那些符合<join-condition>的行才会被记录在虚表VT2中。
3.`JOIN`： 如果指定了OUTER JOIN（比如left join、 right join），那么保留表中未匹配的行就会作为外部行添加到虚拟表VT2中，产生虚拟表VT3, rug from子句中包含两个以上的表的话，那么就会对上一个join连接产生的结果VT3和下一个表重复执行步骤1~3这三个步骤，一直到处理完所有的表为止。
4.`WHERE`： 对虚拟表VT3进行WHERE条件过滤。只有符合<where-condition>的记录才会被插入到虚拟表VT4中。
5.`GROUP BY`: 根据group by子句中的列，对VT4中的记录进行分组操作，产生VT5.
6.`CUBE | ROLLUP`: 对表VT5进行cube或者rollup操作，产生表VT6.
7.`HAVING`： 对虚拟表VT6应用having过滤，只有符合<having-condition>的记录才会被 插入到虚拟表VT7中。
8.`SELECT`： 执行select操作，选择指定的列，插入到虚拟表VT8中。
9.`DISTINCT`： 对VT8中的记录进行去重。产生虚拟表VT9.
10.`ORDER BY`: 将虚拟表VT9中的记录按照<order_by_list>进行排序操作，产生虚拟表VT10.
11.`LIMIT`：取出指定行的记录，产生虚拟表VT11, 并将结果返回。

# 不使用COUNT(*)加LIMIT，如何获取分页数据及总记录数？

在很多分页的程序中都这样写:

```
SELECT COUNT(*) from ‘table’ WHERE ......;    # 查出符合条件的记录总数
SELECT * FROM ‘table’ WHERE ...... limit M,N;   # 查询当页要显示的数据
```
这样的语句可以改成:
```
SELECT SQL_CALC_FOUND_ROWS * FROM ‘table’ WHERE ......  limit M, N;
SELECT FOUND_ROWS();
```
这样只要执行一次较耗时的复杂查询可以同时得到与不带limit同样的记录条数。
第二个SELECT返回一个数字，指示了在没有LIMIT子句的情况下，第一个SELECT返回了多少行。

# 两大类触发器

`DML触发器`是基于表而创建的，可以在一张表创建多个DML触发器。其特点是定义在表或者视图上、自动触发、不能被直接调用。用户可以针对INSERT、UPDATE、DELETE语句分别设置触发器，也可以针对一张表上的特定操作设置。触发器可以容纳非常复杂的SQL语句，但不管操作多么复杂，也只能作为一个独立的单元被执行、看作一个事务。如果在执行触发器的过程中发生了错误，则整个事务都会回滚。
`DDL触发器`是一种特殊的触发器，它在响应数据定义语言(DDL)语句时触发。可以用于在数据库中执行管理任务，例如审核以及规范数据库操作。

# 主键与唯一索引的区别

primary key与unique的区别
`UNIQUED 可空`，可以在一个表里的一个或多个字段定义；PRIMARY KEY 不可空不可重复，在一个表里可以定义联合主键；
在一个表中只能有一个Primary Key，而`多个Unique Key可以同时存在`。 
Primary Key一般在逻辑设计中用作记录标识，这也是设置Primary Key的本来用意，而Unique Key只是为了保证域/域组的唯一性。

# 什么是MySQL的行级锁？

如果是InnoDB引擎，就可以在事务里使用行锁，格式为：

```
SELECT xx FROM xx [FORCE INDEX(PRIMARY)] WHERE xx FOR UPDATE 
```
被加锁的行，其他事务也能读取但如果想写的话就必须等待锁的释放。
`只有查询用到的是主键索引或满足最左前缀的主键索引的一部分，并且具有明确的值`，如：
索引列=值、索引列=值1 or 索引列=值2、索引列 IN(值1,值2)
才能实现行锁定，否则就会锁表。
注：`非主键索引会锁表，如果有多个索引可指定用主键索引（FORCE INDEX(PRIMARY)），以免锁表`。

# 冷备份和热备份有什么不同？各自的优缺点是什么？

冷备份
发生在数据库已经正常关闭的情况下，当正常关闭时会提供给我们一个完整的数据库。冷备份是将关键性文件拷贝到另外位置的一种说法。对于备份数据库信息而言，冷备份是最快和最安全的方法。
优点： 
1．是非常快速的备份方法（只需拷贝文件） 
2．容易归档（简单拷贝即可） 
3．容易恢复到某个时间点上（只需将文件再拷贝回去） 
4．能与归档方法相结合，做数据库“最新状态”的恢复。 
5．低度维护，高度安全。 

缺点： 
1．单独使用时，只能提供到“某一时间点上”的恢复。 
2．在实施备份的全过程中，数据库必须要作备份而不能作其它工作。也就是说，在冷备份过程中，数据库必须是关闭状态。 
3．若磁盘空间有限，只能拷贝到磁带等其它外部存储设备上，速度会很慢。 
4．不能按表或按用户恢复。 
值得注意的是冷备份必须在数据库关闭的情况下进行，当数据库处于打开状态时，执行数据库文件系统备份是无效的 。而且在恢复后一定要把数据库文件的属组和属主改为mysql。

热备份
在数据库运行的情况下，备份数据库操作的sql语句，当数据库发生问题时，可以重新执行一遍备份的sql语句。

优点： 
1．可在表空间或数据文件级备份，备份时间短。 
2．备份时数据库仍可使用。 
3．可达到秒级恢复（恢复到某一时间点上）。 
4．可对几乎所有数据库实体作恢复。 
5．恢复是快速的，在大多数情况下在数据库仍工作时恢复。 

缺点： 
1．不能出错，否则后果严重。 
2．若热备份不成功，所得结果不可用于时间点的恢复。 
3．因难于维护，所以要特别仔细小心，不允许“以失败而告终”。

MySQL原生支持多机热备。

# 在进行数据库编程时，连接池有什么作用？

由于创建连接和释放连接都有很大的开销（尤其是数据库服务器不在本地时，每次建立连接都需要进行TCP的三次握手，再加上网络延迟，造成的开销是不可忽视的），为了提升系统访问数据库的性能，可以事先创建若干连接置于连接池中，需要时直接从连接池获取，使用结束时归还连接池而不必关闭连接，从而避免频繁创建和释放连接所造成的开销，这是典型的用空间换取时间的策略（浪费了空间存储连接，但节省了创建和释放连接的时间）。池化技术在Java开发中是很常见的，在使用线程时创建线程池的道理与此相同。基于Java的开源数据库连接池主要有： C3P0、Proxool、DBCP、BoneCP、Druid等。
【补充】在计算机系统中时间和空间是不可调和的矛盾，理解这一点对设计满足性能要求的算法是至关重要的。大型网站性能优化的一个关键就是使用缓存，而缓存跟上面讲的连接池道理非常类似，也是使用空间换时间的策略。可以将热点数据置于缓存中，当用户查询这些数据时可以直接从缓存中得到，这无论如何也快过去数据库中查询。当然，缓存的置换策略等也会对系统性能产生重要影响，对于这个问题的讨论已经超出了这里要阐述的范围。

# 外联接

LEFT OUTER JOIN或LEFT JOIN \ RIGHT OUTER JOIN或RIGHT JOIN \ FULL OUTER JOIN或FULL JOIN(左外联接的结果集中除了包括满足条件的行外，还包括左表所有的行(左表中没有满足条件的以空值的形式出现))

# 如何为SELECT语句添加一个自动增加的列？

```
set @N = 0;
SELECT @N := @N +1 AS number, name, surname FROM gbtags_users;
```

# 如何分析MySQL语句执行时间和消耗资源？

```
SET profiling=1;          # 启动profiles，默认是没开启的
SELECT * FROM customers;      # 执行要分析的SQL语句

SHOW profiles;            # 查看SQL语句具体执行步骤及耗时

SHOW profile cpu,block io FOR QUERY 41;   # 查看ID为41的查询在各个环节的耗时和资源消耗
```

# 如何分析MySQL语句的执行情况？

```
mysql> explain select * from t_online_group_records where UNIX_TIMESTAMP(gre_updatetime) > 123456789;
+----+-------------+------------------------+------+---------------+------+---------+------+------+-------------+
| id | select_type | table                  | type | possible_keys | key  | key_len | ref  | rows | `Extra`      |
+----+-------------+------------------------+------+---------------+------+---------+------+------+-------------+
|  1 | SIMPLE      | t_online_group_records | ALL  | NULL          | NULL | NULL    | NULL |   47 | Using where |
+----+-------------+------------------------+------+---------------+------+---------+------+------+-------------+
1 row in set (0.00 sec)
```
如上面例子所示，重点关注下type，rows和Extra：
`type`：`操作的类型`，可以用来判断有无使用到索引。结果值从好到坏：`... > range(使用到索引) > index > ALL(全表扫描)`，一般查询应达到range级别，具体可能值如下：
  `SYSTEM`  # CONST的特例，当表上只有一条记录匹配
  `CONST` # WHERE条件筛选后表上至多有一条记录匹配时，比如WHERE ID = 2 （ID是主键，值为2的要么有一条要么没有）
  `EQ_REF`  # 参与连接运算的表是内表（在代码实现的算法中，两表连接时作为循环中的内循环遍历的对象，这样的表称为内表）。
基于索引（连接字段上存在唯一索引或者主键索引，且操作符必须是“=”谓词，索引值不能为NULL）做扫描，使得对外表的一条元组，内表只有唯一一条元组与之对应。
  `REF`   # 可以用于单表扫描或者连接。参与连接运算的表，是内表。
基于索引（连接字段上的索引是非唯一索引，操作符必须是“=”谓词，连接字段值不可为NULL）做扫描，使得对外表的一条元组，内表可有若干条元组与之对应。
  `REF_OR_NULL`   # 类似REF，只是搜索条件包括：连接字段的值可以为NULL的情况，比如 where col = 2 or col is null
  `RANGE`     # `范围扫描`，基于索引做范围扫描，为诸如BETWEEN，IN，>=，LIKE类操作提供支持
  `INDEX_SCAN`    # 索引做扫描，是基于索引在索引的叶子节点上找满足条件的数据（不需要访问数据文件）
  `ALL`       # `全表扫描`或者范围扫描：不使用索引，顺序扫描，直接读取表上的数据（访问数据文件）
  `UNIQUE_SUBQUERY` # 在子查询中，基于唯一索引进行扫描，类似于EQ_REF
  `INDEX_SUBQUERY`    # 在子查询中，基于除唯一索引之外的索引进行扫描
  `INDEX_MERGE`   # 多重范围扫描。两表连接的每个表的连接字段上均有索引存在且索引有序，结果合并在一起。适用于作集合的并、交操作。
  `FT`        # FULL TEXT，`全文检索`
rows：SQL执行检查的记录数
Extra：SQL执行的附加信息，如`"Using index"表示查询只用到索引列`，不需要去读表等

# 如何在查询时重新定义数值类型？

使用CASE来重新定义数值类型

```
SELECT id,title,(CASE date WHEN '0000-00-00' THEN '' ELSE date END) AS date
FROM your_table
  
SELECT id,title,
(CASE status WHEN 0 THEN 'open' WHEN 1 THEN 'close' ELSE 'standby' END) AS status
FROM your_table
```

# 如何理解数据库设计的三个范式？

通俗地理解三个范式，对于数据库设计大有好处。在数据库设计中，为了更好地应用三个范式，就必须通俗地理解 三个范式(通俗地理解是够用的理解，并不是最科学最准确的理解)： 
第一范式：1NF是对属性的`原子性`约束，要求属性具有原子性，不可再分解； 
第二范式：2NF是对记录的`惟一性`约束，要求记录有惟一标识，即实体的惟一性； 
第三范式：3NF是对`字段冗余性`的约束，即任何字段不能由其他字段派生出来，它要求字段没有冗余。 

没有冗余的数据库设计可以做到。但是，`没有冗余的数据库未必是最好的数据库`，有时为了提高运行效率，就必须降低范式标准，适当保留冗余数据。具体做法是：在概念数据模型设计时遵守第三范式，降低范式标准的工作放到物理数据模型设计时考虑。降低范式就是增加字段，允许冗余。

范式是符合某一种级别的关系模式的集合。关系数据库中的关系必须满足一定的要求，即满足不同的范式。目前关系数据库有六种范式。一般说来，数据库只需满足第三范式（3NF）就行了。
第一范式（1NF）：指数据库表的每一列都是不可分割的基本数据项，`同一列中不能有多个值`，即实体中的某个属性不能有多个值或者不能有重复的属性。例如，不能将员工信息都放在一列中显示，也不能将其中的两列或多列在一列中显示；员工信息表的每一行只表示一个员工的信息，一个员工的信息在表中只出现一次。简而言之，第一范式就是无重复的列。
第二范式（2NF）：第二范式（2NF）要求数据库表中的每个实例或`行必须可以被惟一地区分`。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。简而言之，第二范式就是非主属性的部分依赖于主关键字。
第三范式（3NF）：第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。例如，存在一个部门信息表，其中每个部门有部门编号（dept_id）、部门名称、部门简介等信息。那么在员工信息表中列出部门编号后就不能再将部门名称、部门简介等与部门有关的信息再加入员工信息表中。如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余。简而言之，第三范式就是属性不依赖于其它非主属性。

# 如何进行MySQL数据备份与恢复操作？

（1）备份：使用`mysqldump`导出数据

```
mysqldump -u 用户名 -p 数据库名 [表名] > 导出的文件名
mysqldump -uroot -p test mytable > mytable.20140921.bak.sql
```

（2）恢复：导入备份数据
```
mysql -uroot -p test < mytable.20140921.bak.sql
```

（3）恢复：导入备份数据之后发送的写操作。先使用mysqlbinlog导出这部分写操作SQL(基于时间点或位置)
// 导出2014-09-21 09:59:59之后的binlog：
```
mysqlbinlog --database="test" --start-date="2014-09-21 09:59:59" /var/lib/mysql/mybinlog.000001 > binlog.data.sql

// 导出起始id为123456之后的binlog：
mysqlbinlog --database="test" --start-position="123456" /var/lib/mysql/mybinlog.000001 > binlog.data.sql

// 把要恢复的binlog导入db
mysql -uroot -p test < binlog.data.sql
```

# 存储过程的概念以及优缺点是什么？

存储过程是一套已经预先编译好的SQL代码,是SQL语句和可选控制语句的集合及一个独立的数据库对象.存储过程在数据库内可以由应用程序调用执行,而且允许用户声明变量、有条件执行以及其他强大的编程工程。由于存储过程是已经编译好的代码，所以执行的时候不需要分析也不需要再次编译，能够提高程序的运行效率。
存储过程可以包含程序流、逻辑以及对数据库的查询。可以接受参数、输出参数、返回单个或者多个结果集以及返回值。
带简单参数的存储过程

```sql
/*带学号参数的存储过程*/
CREATE   PROCEDURE  s
@id   int/*参数*/
AS
SELECT   *    FROM  student
WHERE id=@id
GO
```
```
/*输入参数2001002的学生号，查询学号2001002学生的信息*/
s  2001002/调用形式/
GO
```
优点补充：
存储过程可以用于`降低网络流量`，存储过程代码直接存储于数据库中，所以不会产生大量T-sql语句的代码流量。
通过向用户授予对存储过程（而不是基于表）的访问权限，它们可以提供对特定数据的访问
缺点： 
1.如果更改范围大到需要对输入存储过程的参数进行更改，或者要更改由其返回的数据，则您仍需要更新程序集中的代码以添加参数、更新 GetValue() 调用，等等，这时候估计比较繁琐了。 
2.可移植性差：由于存储过程将应用程序绑定到 SQL Server，因此使用存储过程封装业务逻辑将限制应用程序的可移植性。如果应用程序的可移植性在您的环境中非常重要，则将业务逻辑封装在不特定于 RDBMS 的中间层中可能是一个更佳的选择。 
3. 大量采用存储过程进行业务逻辑的开发致命的缺点是很多存储过程不支持面向对象的设计，无法采用面向对象的方式将业务逻辑进行封装，从而无法形成通用的可支持复用的业务逻辑框架。

# 存在主键冲突时就更新

ON DUPLICATE KEY UPDATE语句的作用是什么？

```
INSERT INTO ... ON DUPLICATE KEY UPDATE col=VALUES(col)   # VALUES用来取插入的值，存在主键冲突时就更新，没有删除操作
```

例：更新统计表
老做法是写三条sql语句:
```
select * from player_count where player_id = 1;         # 查询统计表中是否有记录
insert into player_count(player_id,count) value(1,1);       # 没有记录就执行insert 操作
update player_count set count = count+1 where player_id = 1;    # 有记录就执行update操作
```
这种写法比较麻烦，用on duplicate key update 的做法如下：
```
insert into player_count(player_id,count) value(1,1) on duplicate key update count=count+1;
```

# 慢查询记录格式

可以查看每个慢查询SQL的耗时
User@Host: edu_online[edu_online] @  [10.139.10.167]
Query_time: 1.958000  Lock_time: 0.000021 Rows_sent: 254786  Rows_examined: 254786
SET timestamp=1410883292;
select * from t_online_group_records;
日志显示该查询用了1.958秒，返回254786行记录，一共遍历了254786行记录。及具体的时间戳和SQL语句。

使用`mysqldumpslow`进行慢查询日志分析：
输入：
mysqldumpslow -s t -t 5 slow_log_20140819.txt 
-s：排序方法，t表示按时间（此外，c为按次数，r为按返回记录数等）
-t：取Top多少条，-t 5表示取前5条

输出：
Count: 1076100  Time=0.09s (99065s)  Lock=0.00s (76s)  Rows=408.9 (440058825), edu_online[edu_online]@28hosts
  select * from t_online_group_records where UNIX_TIMESTAMP(gre_updatetime) > N
Count: 1076099  Time=0.05s (52340s)  Lock=0.00s (91s)  Rows=62.6 (67324907), edu_online[edu_online]@28hosts
  select * from t_online_course where UNIX_TIMESTAMP(c_updatetime) > N
Count: 63889  Time=0.78s (49607s)  Lock=0.00s (3s)  Rows=0.0 (18), edu_online[edu_online]@[10.213.170.137]
  select f_uin from t_online_student_contact where f_modify_time > N
...
以第1条为例，表示这类SQL（N可以取很多值，这里mysqldumpslow会归并起来）在8月19号的慢查询日志内出现了1076100次，总耗时99065秒，总返回440058825行记录，有28个客户端IP用到。
通过慢查询日志分析，就可以找到最耗时的SQL，然后进行具体的SQL分析了

# 有哪些MySQL性能优化的技巧？

（1）优化MySQL查询语句，使其使用查询缓存
对于相同的查询，MySQL引擎会使用缓存，但是如果在SQL语句中使用函数，如NOW()、RAND()、 CURDATE()等等，则拼凑出的查询不会被认为是相同的查询。
// 查询缓存不开启
$r = mysql_query("SELECT username FROM user WHERE signup_date >= CURDATE()");

// 开启查询缓存
$today = date("Y-m-d");
$r = mysql_query("SELECT username FROM user WHERE signup_date >= '$today'");

（2）当只要一行数据时使用LIMIT 1
这样MySQL数据库引擎会在找到一条数据后停止搜索，而不是继续往后查少下一条符合记录的数据。

（3）为搜索字段建索引
索引并不一定就是给主键或是唯一的字段。如果在表中有某个字段总要会经常用来做搜索，那么请为其建立索引。

（4）在Join表的时候使用相当类型的列，并将其索引
对于那些STRING类型，还需要有相同的字符集才行（两个表的字符集有可能不一样）

（5）千万不要ORDER BY RAND()
You cannot use a column with RAND() values in an ORDER BY clause, because ORDER BY 
would evaluate the column multiple times. 
当记录数据过多时，会非常慢。

（6）避免SELECT *
应该养成一个需要什么就取什么的好的习惯。

（7）使用ENUM而不是VARCHAR
ENUM 实际保存的是TINYINT，但其外表上显示为字符串。如果有一个字段的取值是有限而且固定的，那么，应该使用ENUM而不是VARCHAR。

（8）尽可能的使用NOT NULL
“NULL columns require additional space in the row to record whether their values are NULL. For MyISAM tables, each NULL column takes one bit extra, rounded up to the nearest byte.”
NULL值需要额外的存储空间，而且在比较时也需要额外的逻辑。

（9）把IP地址存成 UNSIGNED INT，而不是VARCHAR(15)

（10）固定长度的表会更快
如果表中的所有字段都是“固定长度”的，整个表会被认为是 “static” 或 “fixed-length”。

（11）垂直分割
“垂直分割”是一种把数据库中的表按列变成几张表的方法，这样可以降低表的复杂度和字段的数目，从而达到优化的目的。
示例一：在Users表中有一个字段是家庭地址，这个字段是可选字段，相比起，而且你在数据库操作的时候除了个人信息外，你`并不需要经常读取或是改写这个字段`。那么，为什么不把他放到另外一张表中呢？ 这样会让你的表有更好的性能，只有用户ID，用户名，口令，用户角色等会被经常使用。小一点的表总是会有好的性能。
示例二： 你有一个叫 “last_login” 的字段，它会在每次用户登录时被更新。但是，每次更新时会导致该表的查询缓存被清空。所以，你可以把这个字段放到另一个表中，这样就不会影响你对用户ID，用户名，用户角色的不停地读取了，因为查询缓存会帮你增加很多性能。
另外，你需要注意的是，这些被分出去的字段所形成的表，你不会经常性地去Join他们，不然的话，这样的性能会比不分割时还要差，而且，会是极数级的下降。
总结：降低表的规模、方便使用缓存、被分出去的字段应该是不会经常要join的字段。

（12）拆分大的 DELETE 或 INSERT 语句
因为这两个操作是会锁表的，表一锁住了，别的操作都进不来了。
如果有一个大的处理，一定把其拆分，使用 LIMIT 条件是一个好的方法。下面是一个示例：
```
while (1) {
//每次只做1000条
mysql_query("DELETE FROM logs WHERE log_date <= '2009-11-01' LIMIT 1000");
if (mysql_affected_rows() == 0) {
    // 没得可删了，退出！
    break;
}
// 每次都要休息一会儿
usleep(50000);
}
```

（13）越小的列会越快
如使用TINYINT而不是INT，使用DATE而不是DATETIME。

（14）选择一个正确的存储引擎
MyISAM 适合于一些需要大量查询的应用，但其对于有大量写操作并不是很好。甚至你只是需要update一个字段，整个表都会被锁起来，而别的进程，就算是读进程都无法操作直到读操作完成。另外，MyISAM 对于 SELECT COUNT(*) 这类的计算是超快无比的。
InnoDB 的趋势会是一个非常复杂的存储引擎，对于一些小的应用，它会比 MyISAM 还慢。他是它支持“行锁” ，于是在写操作比较多的时候，会更优秀。并且，他还支持更多的高级应用，比如：事务。

（15）小心“永久链接”
“永久链接”的目的是用来减少重新创建MySQL链接的次数。当一个链接被创建了，它会永远处在连接的状态，就算是数据库操作已经结束了。而且，自从Apache开始重用它的子进程后——也就是说，下一次的HTTP请求会重用Apache的子进程，并重用相同的 MySQL 链接。
但是从个人经验上来说，这个功能制造出来的麻烦事更多。因为，你只有有限的链接数，内存问题，文件句柄数，等等。

（1）对接触的项目进行慢查询分析，发现TOP10的基本都是忘了加索引或者索引使用不当，如索引字段上加函数导致索引失效等(如where UNIX_TIMESTAMP(gre_updatetime)>123456789)

（2）另外很多同学在拉取全表数据时，喜欢用select xx from xx limit 5000,1000这种形式批量拉取，其实这个SQL每次都是全表扫描，建议添加1个自增id做索引，将SQL改为`select xx from xx where id>5000 and id<6000`;

只select出需要的字段，避免select *

尽量早做过滤，使Join或者Union等后续操作的数据量尽量小

（5）把能在逻辑层算的提到逻辑层来处理，如一些数据排序、时间函数计算等

# 编码设置

```
SHOW VARIABLES LIKE 'character_set_%';

Variable_name             Value                             
------------------------  ----------------------------------
character_set_client      utf8mb4                           
character_set_connection  utf8mb4                           
character_set_database    utf8mb4                           
character_set_filesystem  binary                            
character_set_results     utf8mb4                           
character_set_server      utf8mb4                           
character_set_system      utf8                              
character_sets_dir        /usr/local/mysql/share/charsets/  
```

# 覆盖索引

什么是Covering Index？如何使用？如何判断索引是否生效？
对于SELECT a FROM … WHERE b = …这种查询，通常的做法是在b字段上建立索引，执行查询时系统会查询b索引进行定位，然后再利用此定位去表里查询需要的数据a。即该过程存在两次查询，一次是查询索引，一次是查询表。
使用Covering Index可以只查询一次索引就完成。建立一个组合索引’b,a’，当查询时，通过组合索引的b部分去定位，至于需要的数据a，立刻就可以在索引里得到，从而省略了表查询的过程。
如果使用Covering Index，要注意SELECT的方式，只SELECT必要的字段，而不能SELECT *，因为不太可能把所有的字段一起做索引。
可以`使用EXPLAIN命令来确认是否使用了组合索引`：如果在Extra里出现Using Index，就说明使用的是Covering Index。
实例1：

```
SELECT COUNT(*) FROM articles WHERE category_id = …
```
当在category_id建立索引后，这个查询使用的就是Covering Index。

实例2：
比如说在文章系统里分页显示的时候，一般的查询是这样的：
```
SELECT id, title, content FROM article ORDER BY created DESC LIMIT 10000, 10;
```
通常这样的查询会把索引建在created字段（其中id是主键），不过当LIMIT偏移很大时，查询效率仍然很低，改变一下查询：
```
SELECT id, title, content FROM article
INNER JOIN (
SELECT id FROM article ORDER BY created DESC LIMIT 10000, 10
) AS page USING(id)
```
此时，建立复合索引”created, id”就可以在子查询里利用上Covering Index，快速定位id。

# 视图

也被称为虚拟的表，其内容由SELECT查询语句定义。同真实的表一样，视图包含了一系列带有名称的列和行的数据。但是，视图并不在数据库中以存储的数据集合形式存在。用行和列的数据，来自由定义视图的查询所引用的表，并且在引用视图时动态生成。视图一经定义，便存储在数据库中，与其相对应的数据并没有像表那样又在数据库中再存储一份。通过视图看到的数据只是存放在基表中的数据。对视图的操作与对表的操作一样，可以查询、修改、删除。通过对视图看到的数据进行修改时，相应的基表的数据也要发生变化，同时，若基表的数据发生变化，这种变化也可以自动地反映到视图中。
视图和查询最主要的差别是：视图的存储是作为数据库开发者设计数据库的一部分；而查询仅仅是对表的查询并非数据库设计的一部分。

# B树

B树是对二叉查找树的改进。它的设计思想是，将相关数据尽量集中在一起，以便一次读取多个数据，减少硬盘操作次数。
特点如下：
（1）一个节点可以容纳多个值。比如上图中，最多的一个节点容纳了4个值。
（2）除非数据已经填满，否则不会增加新的层。也就是说，`B树追求"层"越少越好`。
（3）子节点中的值，与父节点中的值，有严格的大小对应关系。一般来说，`如果父节点有a个值，那么就有a+1个子节点`。比如上图中，父节点有两个值（7和16），就对应三个子节点，第一个子节点都是小于7的值，最后一个子节点都是大于16的值，中间的子节点就是7和16之间的值。
这种数据结构，非常有利于减少读取硬盘的次数。`假定一个节点可以容纳100个值，那么3层的B树可以容纳100万个数据`，如果换成二叉查找树，则需要20层！假定操作系统一次读取一个节点，并且根节点保留在内存中，那么B树在100万个数据中查找目标值，只需要读取两次硬盘。
数据库以B树格式储存，只解决了按照"主键"查找数据的问题。如果想查找其他字段，就需要建立索引（index）。所谓索引，就是以某个字段为关键字的B树文件（索引的一种）。

# groupBy注意点

1.在select指定的字段要么就要包含在Group By语句的后面，作为分组的依据；要么就要被包含在聚合函数中；
2.having 子句的作用是筛选满足条件的组，即`在分组之后过滤数据`，条件中经常包含聚合函数，使用having 条件过滤出特定的组，也可以使用多个分组标准进行分组。


# innodb_buffer_pool_size设置什么？

innodb_buffer_pool_size这个参数主要作用是缓存`innodb表的索引、数据、插入数据时的缓冲`。
默认值：128M
专用mysql服务器设置的大小： 操作系统内存的70%-80%最佳。
设置方法：
my.cnf文件
innodb_buffer_pool_size = 6G
此外，这个参数是非动态的，要修改这个值，需要重启mysqld服务。

如果因为内存不够，MySQL无法启动，就会在错误日志中出现如下报错：
InnoDB: mmap(137363456 bytes) failed; errno 12

# LIMIT语句起始行号如何计算？有无替代方法？

返回不多于5行（小于等于）

```
SELECT prod_name
FROM products
LIMIT 5;
```

返回从第6行开始的5行（行号从0开始）
```
SELECT prod_name
FROM products
LIMIT 5,5;
```

返回从第6行开始的5行（LIMIT的一种替代语法）
```
SELECT prod_name
FROM products
LIMIT 5 OFFSET 5;
```

# MyISAM和InnoDB的区别

1.MySQL默认采用的是MyISAM。

2.MyISAM不支持`事务`，而InnoDB支持。InnoDB的AUTOCOMMIT默认是打开的，即每条SQL语句会默认被封装成一个事务，自动提交，这样会影响速度，所以最好是把多条SQL语句显示放在begin和commit之间，组成一个事务去提交。

3.InnoDB支持数据`行级锁`，MyISAM不支持行锁定，只支持锁定整个表。即MyISAM同一个表上的读锁和写锁是互斥的，MyISAM并发读写时如果等待队列中既有读请求又有写请求，默认写请求的优先级高，即使读请求先到，所以MyISAM不适合于有大量查询和修改并存的情况，那样查询进程会长时间阻塞。因为MyISAM是锁表，所以某项读操作比较耗时会使其他写进程饿死。

4.InnoDB支持`外键`，MyISAM不支持。

5.InnoDB的`主键范围`更大，最大是MyISAM的2倍。

6.InnoDB不支持`全文索引`，而MyISAM支持。全文索引是指对char、varchar和text中的每个词（停用词除外）建立倒排序索引。`MyISAM的全文索引其实没啥用，因为它不支持中文分词`，必须由使用者分词后加入空格再写到数据表里，而且少于4个汉字的词会和停用词一样被忽略掉。

7.MyISAM支持GIS数据，InnoDB不支持。即MyISAM支持以下空间数据对象：Point,Line,Polygon,Surface等。

8.没有where的count(*)使用MyISAM要比InnoDB快得多。因为MyISAM`内置了一个计数器`，count(*)时它直接从计数器中读，而InnoDB必须扫描全表。所以在InnoDB上执行count(*)时一般要伴随where，且where中要包含主键以外的索引列。为什么这里特别强调“主键以外”？因为InnoDB中primary index是和raw data存放在一起的，而secondary index则是单独存放，然后有个指针指向primary key。所以只是count(*)的话使用secondary index扫描更快，而primary key则主要在扫描索引同时要返回raw data时的作用较大。

# MySQL一行记录的最大长度是多少？

MySQL表中一行的长度不能超过65535字节，VARCHAR(N)使用额外的1到2字节来存储值的长度，如果N<=255，则使用一个字节，否则使用两个字节；如果表格的编码为UTF8（一个字符占3个字节），那么VARCHAR(255)占用的字节数为255 * 3 + 2 = 767，这样，一行就最多只能有65535 / 765 = 85个VARCHAR(255)类型的列。

# MySQL中有哪几种事务隔离级别？

1）`READ UNCOMMITTED`  # 可读取其他事务未提交的数据（脏读）
2）`READ COMMITTED`    # 只能读取已提交的数据，但是不可重复读（避免脏读）
3）`REPEATABLE READ`   # 可重复读
// 用户A查询完之后，用户B将无法更新用户A所查询到的数据集中的任何数据（但是可以更新、插入和删除用户A查询到的数据集之外的数据），直到用户A事务结束才可以进行更新，这样就有效的防止了用户在同一个事务中读取到不一致的数据。
4）`SERIALIZABLE`      # 事务串行化，必须等待当前事务执行完，其他事务才可以执行写操作，`有多个事务同时设置SERIALIZABLE时会产生死锁`：
ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction
这是四个隔离级别中限制最大的级别。因为并发级别较低，所以应只在必要时才使用该选项。

使用事务时设置级别：
START TRANSACTION
SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE}
COMMIT
ROLLBACK


# MySQL如何创建分区表？

分区是一种粗粒度，简易的索引策略，适用于大数据的过滤场景。`对于大数据(如10TB)而言，索引起到的作用相对小`，因为索引的空间与维护成本很高，另外如果不是索引覆盖查询，将导致回表，造成大量磁盘IO。
`分区表分为RANGE、LIST、HASH、KEY四种类型`,并且分区表的索引是可以局部针对分区表建立的。
用RANGE创建分区表：

```
CREATE TABLE sales (
  id INT AUTO_INCREMENT,
  amount DOUBLE NOT NULL,
  order_day DATETIME NOT NULL,
  PRIMARY KEY(id, order_day)
) ENGINE=Innodb PARTITION BY RANGE(YEAR(order_day)) (
  PARTITION p_2010 VALUES LESS THAN (2010),
  PARTITION p_2011 VALUES LESS THAN (2011),
  PARTITION p_2012 VALUES LESS THAN (2012),
  PARTITION p_catchall VALUES LESS THAN MAXVALUE);
```
如果这么做，则order_day必须包含在主键中，且会产生一个问题：当年份超过阈值，到了2013，2014时需要手动创建这些分区，更好的方法是使用HASH：
```
CREATE TABLE sales ( 
  id INT PRIMARY KEY AUTO_INCREMENT,
  amount DOUBLE NOT NULL,
  order_day DATETIME NOT NULL
) ENGINE=Innodb PARTITION BY HASH(id DIV 1000000);
```
这种分区表示每100W条数据建立一个分区，且没有阈值范围的影响。

`如果想为一个表创建分区，这个表最多只能有一个唯一索引`（主键也是唯一索引）。如果没有唯一索引，可指定任何一列为分区列；否则就只能指定唯一索引中的任何一列为分区列。查询时需用到分区的列，不然会遍历所有的分区，比不分区的查询效率还低，MySQL支持子分区。
在表建立后也可以新增、删除、合并分区。

# MySQL如何进行主从数据同步？

复制机制（Replication）
master通过复制机制，将master的写操作通过binlog传到slave生成中继日志(relaylog)，slave再将中继日志redo，使得主库和从库的数据保持同步

复制相关的3个Mysql线程（属于slave主动请求拉取的模式）
（1）slave上的I/O线程：向master请求数据
（2）master上的Binlog Dump线程：读取binlog事件并把数据发送给slave的I/O线程
（3）slave上的SQL线程：读取中继日志并执行，更新数据库

相关监控命令
show processlist    # 查看MySQL进程信息，包括3个同步线程的当前状态
show master status    # 查看master配置及当前复制信息
show slave status   # 查看slave配置及当前复制信息

# MySQL如何进行慢查询日志分析？

慢查询相关的配置参数：
log_slow_queries          # 是否打开慢查询日志，得先确保=ON后面才有得分析
long_query_time             # 查询时间大于多少秒的SQL被当做是慢查询，一般设为1S
log_queries_not_using_indexes   # 是否将没有使用索引的记录写入慢查询日志
slow_query_log_file           # 慢查询日志存放路径

# MySQL异步

MySQL异步是指将MySQL连接事件驱动化，这样就编程了非阻塞IO。数据库操作并不会阻塞进程，在MySQL-Server返回结果时再执行对应的逻辑。
有几个点需要注意一下：
异步MySQL并没有节省SQL执行的时间
一个MySQL连接同时只能执行1个SQL，如果异步MySQL存在并发那么必须创建多个MySQL连接
异步回调程序中，异步MySQL并没有提升性能。`异步最大的好处是可以高并发`，如果并发1万个请求，那么就需要建立1万个MySQL连接，这会给MySQL-Server带来巨大的压力。

MySQL是根据连接数分配资源的，一个连接需要开启一个线程。1000连接那么需要维持1000线程才可以。线程数量增加后，线程间切换会占用大量CPU资源
MySQL短连接反而不会出现此问题，因为短连接在使用完后就释放了。不会占用MySQL-Server的连接资源
虽然应用层代码使用异步回调避免了自身的阻塞，实际上真正的瓶颈是数据库服务器。异步MySQL还带来了额外的编程复杂度，所以除非是特殊场景的需求，否则不建议使用异步MySQL。
如果程序中坚持要使用异步，那么必须是`异步MySQL+连接池`的形式。超过规定的MySQL最大连接后，应当对SQL请求进行排队，而不是创建新连接，避免大量并发请求导致MySQL服务器崩溃。

# MySQL性能相关的配置参数有哪些？

`max_connecttions`：最大连接数
`table_cache`：缓存打开表的数量
`key_buffer_size`：索引缓存大小
`query_cache_size`：查询缓存大小
`sort_buffer_size`：排序缓存大小(会将排序完的数据缓存起来)
`read_buffer_size`：顺序读缓存大小
`read_rnd_buffer_size`：某种特定顺序读缓存大小(如order by子句的查询)
查看配置方法：show variables like '%max_connecttions%';

# MySQL的索引有哪些类型？

索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们`包含着对数据表里所有记录的引用指针`。有了相应的索引之后，数据库会直接在索引中查找符合条件的选项。有以下索引类型：
（1）`普通索引`
这是最基本的索引，它没有任何限制，`MyIASM中默认的BTREE类型的索引`，也是大多数情况下用到的索引。
// 直接创建索引

```
CREATE INDEX index_name ON table(column(length))

// 修改表结构的方式添加索引
ALTER TABLE table_name ADD INDEX index_name ON (column(length))

// 创建表的时候同时创建索引
CREATE TABLE `table` (
`id` int(11) NOT NULL AUTO_INCREMENT ,
`title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL ,
`content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL ,
`time` int(10) NULL DEFAULT NULL ,
PRIMARY KEY (`id`),
INDEX index_name (title(length))
)
// 删除索引
DROP INDEX index_name ON table
```

（2）`唯一索引`
索引列的值必须唯一，但允许有空值（注意和主键不同）。如果是组合索引，则列值的组合必须唯一，创建方法和普通索引类似。
```
// 创建唯一索引
CREATE UNIQUE INDEX indexName ON table(column(length))

// 修改表结构
ALTER TABLE table_name ADD UNIQUE indexName ON (column(length))

// 创建表的时候直接指定
CREATE TABLE `table` (
`id` int(11) NOT NULL AUTO_INCREMENT ,
`title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL ,
`content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL ,
`time` int(10) NULL DEFAULT NULL ,
PRIMARY KEY (`id`),
UNIQUE indexName (title(length))
);
```

（3）`全文索引`
`仅可用于MyISAM表`。可以从CHAR、VARCHAR或TEXT列中作为CREATE TABLE语句的一部分被创建，或是随后使用ALTER TABLE或CREATE INDEX被添加。对于较大的数据集，将资料输入一个没有FULLTEXT索引的表中，然后创建索引，其速度比把资料输入现有FULLTEXT索引的速度更为快。不过切记对于大容量的数据表，生成全文索引是一个非常消耗时间非常消耗硬盘空间的做法。
```
// 创建表的适合添加全文索引
CREATE TABLE `table` (
`id` int(11) NOT NULL AUTO_INCREMENT ,
`title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL ,
`content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL ,
`time` int(10) NULL DEFAULT NULL ,
PRIMARY KEY (`id`),
FULLTEXT (content)
);

// 修改表结构添加全文索引
ALTER TABLE article ADD FULLTEXT index_content(content)

// 直接创建索引
CREATE FULLTEXT INDEX index_content ON article(content)
```

（4）`单列索引`、`多列索引`
多个单列索引与单个多列索引的查询效果不同，因为执行查询时，MySQL只能使用一个索引，会从多个索引中选择一个限制最为严格的索引。

（5）`组合索引`
例如上表中针对title和time建立一个组合索引：
ALTER TABLE article ADD INDEX index_titme_time (title(50),time(10));
建立这样的组合索引，其实是`相当于分别建立了下面两组组合索引`：
–title,time
–title
为什么没有time这样的组合索引呢？这是因为MySQL`组合索引“最左前缀”`的结果。简单的理解就是只从最左面的开始组合。如下面的几个SQL所示：
```
// 使用到上面的索引
SELECT * FROM article WHREE title='测试' AND time=1234567890;   # 使用title,time索引
SELECT * FROM article WHREE utitle='测试';              # 使用title索引

// 不使用上面的索引
SELECT * FROM article WHREE time=1234567890;            
```
MySQL只对以下操作符才使用索引：<,<=,=,>,>=,between,in,以及某些时候的like(不以通配符%或_开头的情形)。而理论上每张表里面最多可创建16个索引。



# MySQL自增主键出现不连续情况的原因？如何修复？

A、使用insert into插入数据时，若abc的值已存在，因其为唯一键，故不会插入成功。但此时，那个AUTO_INCREMENT已然+1了。
eg : insert into `table` set `abc` = '123'


B、使用replace插入数据时，若abc的值已存在，则会先删除表中的那条记录，尔后插入新数据。
eg : replace into `table` set `abc` = '123'
（注：上一行中的into可省略；这只是一种写法。）

  1、insert语句不管是否成功，都会增加AUTO_INCREMENT值。

    2、进行了delete相关操作。



    3、rollback相关。
修复：
```
INSERT INTO th_page2(site,url,title,title_index,content,tag,created_at,updated_at,deleted_at)
SELECT site,url,title,title_index,content,tag,created_at,updated_at,deleted_at FROM th_page ORDER BY tag;

DROP TABLE th_page;

ALTER TABLE th_page2 RENAME th_page;
```

# MySQL连接池

连接池是可以有效降低MySQL-Server负载的。原理是 连接池使用一个共享资源的模式，如并发100个请求，实际上并不是每个请求的所有时间都在执行SQL查询。这样100个请求，共享20个MySQL连接就可以满足需求了。当一个请求操作完数据库后，开始进入模板渲染等流程，这时就会释放数据库连接给其他的请求使用。
`连接池仅在超大型应用中才有价值`。普通的应用采用MySQL长连接方案，每个php-fpm创建一个MySQL连接，每台机器开启100个php-fpm进程。如果有10台机器，每台机器并发的请求为100。实际上只需要创建1000个MySQL连接就能满足需求，数据库的压力并不大。即使有100台机器，硬件配置好的存储服务器依然可以承受。
达到数百或者数千台应用服务器时，MySQL服务器就需要维持十万级的连接。这时数据库的压力就会非常大了。连接池技术就可以派上用场了，可以大大降低数据库连接数。
基于swoole的AsyncTask模块实现的连接池是完美方案，编程简单，没有数据同步和锁的问题。甚至可以多个服务共享连接池。缺点是1, 灵活性不如多线程连接池，无法动态增减连接。2, 有一次进程间通信的开销。
node.js/ngx_lua等在多进程的模式下，无法开发出真正的连接池，除非也像swoole_task这样来实现。

# ON与WHERE有什么区别？

执行连接操作时，可先用ON先进行过滤，减少连接操作的中间结果，然后用WHERE对连接产生的结果再一次过滤。但是，如果是左/右连接，在ON条件里对主表的过滤是无效的，仍然会用到主表的所有记录，连接产生的记录如果不满足主表的过滤条件那么从表部分的数据会置为NULL。



# Redis常见配置选项

Redis的配置文件位于Redis安装目录下，文件名为redis.conf。
$ ./redis-server redis.conf       # 指定配置文件启动redis

redis 127.0.0.1:6379> CONFIG GET loglevel       # 查看loglevel配置项
1) "loglevel"
2) "notice"

redis 127.0.0.1:6379> CONFIG GET *            # 查看所有配置项
1) "dbfilename"
2) "dump.rdb"
3) "requirepass"
4) ""
5) "masterauth"
6) ""
7) "unixsocket"
8) ""

redis 127.0.0.1:6379> CONFIG SET loglevel "notice"    # 设置loglevel项
OK

常见配置参数：
（1）daemonize no           # 启用守护进程，默认不启用

（2）pidfile /var/run/redis.pid     # 指定pid文件，当以守护进程方式运行时用到

（3）port 6379              # 指定Redis监听端口

（4）bind 127.0.0.1           # 绑定的主机地址

（5）timeout 300          # 当客户端闲置指定时间后关闭连接，如果指定为0，表示关闭该功能

（6）loglevel verbose         # 指定日志记录级别：debug、verbose、notice、warning

（7）logfile stdout           # 日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null

（8）databases 16           # 设置数据库的数量，默认数据库为0，可以使用SELECT <dbid>命令在连接上指定数据库id

（9）指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合
    save <seconds> <changes>
    Redis默认配置文件中提供了三个条件：
    save 900 1    # 900秒（15分钟）内有1个更改
    save 300 10   # 300秒（5分钟）内有10个更改
    save 60 10000 # 60秒内有10000个更改

（10）rdbcompression yes        # 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大

（11）dbfilename dump.rdb       # 指定本地数据库文件名，默认值为dump.rdb

（12）dir ./              # 指定本地数据库存放目录

（13）slaveof <masterip> <masterport>   # 设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步

（14）masterauth <master-password>    # 当master服务设置了密码保护时，slav服务连接master的密码

（15）requirepass foobared        # 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH <password>命令提供密码，默认关闭

（16）maxclients 128          # 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息

（17）maxmemory <bytes>         # 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区。

（18）appendonly no           # 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no

（19）appendfilename appendonly.aof   # 指定更新日志文件名，默认为appendonly.aof

（20）appendfsync everysec        #指定更新日志条件，共有3个可选值： 
      no      # 等操作系统进行数据缓存同步到磁盘（快） 
      always    # 每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） 
      everysec  # 每秒同步一次（折衷，默认值）

（21）vm-enabled no           # 指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中

（22）vm-swap-file /tmp/redis.swap    # 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享

（23）vm-max-memory 0           # 将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0

（24）vm-page-size 32           # Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不确定，就使用默认值

（25）vm-pages 134217728        # 设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，在磁盘上每8个pages将消耗1byte的内存。

（26）vm-max-threads 4          # 设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4

（27）glueoutputbuf yes         # 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启

（28）指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法
    hash-max-zipmap-entries 64
    hash-max-zipmap-value 512

（29）activerehashing yes       # 指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍）

（30）include /path/to/local.conf   # 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件

# OAuth2.0授权方式
OAuth是一个关于授权（authorization）的开放网络标准，目前的版本是2.0版。
OAuth的作用就是让"客户端"（第三方应用）安全可控地获取"用户"的授权，与"服务商提供商"（平台，比如微信）进行互动。
OAuth在"客户端"与"服务提供商"之间，设置了一个授权层（authorization layer）。`"客户端"不能直接登录"服务提供商"，只能登录授权层`，以此将用户与客户端区分开来。"客户端"登录授权层所用的令牌（token），与用户的密码不同。用户可以在登录的时候，指定授权层令牌的权限范围和有效期。"客户端"登录授权层以后，`"服务提供商"根据令牌的权限范围和有效期，向"客户端"开放用户储存的资料`。

OAuth 2.0的运行流程如下图：
![image](http://woojean.com/images/net_7.png) 

客户端的授权模式（步骤B）
OAuth 2.0定义了四种授权方式：
（1）`授权码模式`（authorization code） 适用于有server端的应用授权
是功能最完整、流程最严密的授权模式。它的特点就是通过客户端的后台服务器，与"服务提供商"的认证服务器进行互动。
（A）用户访问客户端，后者将前者导向认证服务器。
（B）用户选择是否给予客户端授权。
（C）假设用户给予授权，认证服务器将用户导向客户端事先指定的"重定向URI"（redirection URI），同时附上一个授权码。
（D）客户端收到授权码，附上早先的"重定向URI"，向认证服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。
（E）认证服务器核对了授权码和重定向URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token）。
即：用一个URI去申请，获得用户授权后得到一个对应该URI的授权码。之后就可以用该URI+对应的授权码来获取一个令牌，之后就可以使用该令牌来通过授权层。

（2）`隐式授权`（implicit） 适用于通过客户端访问的应用授权
不通过第三方应用程序的服务器，直接在浏览器中向认证服务器申请令牌，跳过了"授权码"这个步骤，因此得名。所有步骤在浏览器中完成，令牌对访问者是可见的，且客户端不需要认证。
（A）客户端将用户导向认证服务器。
（B）用户决定是否给于客户端授权。
（C）假设用户给予授权，认证服务器将用户导向客户端指定的"重定向URI"，并在URI的Hash部分包含了访问令牌。
（D）浏览器向资源服务器发出请求，其中不包括上一步收到的Hash值。
（E）资源服务器返回一个网页(typically an HTML document with an embedded script)，其中包含的代码可以获取Hash值中的令牌。
（F）浏览器执行上一步获得的脚本，提取出令牌。
（G）浏览器将令牌发给客户端（客户端就可以凭借此令牌来获取数据）。
实例：

其中短暂停留的那个页面的url为：
https://www.zhihu.com/oauth/callback/login/qqconn?code=680726D150FF0B9DF2EBBE2EFEEEC0D4&state=7f13b99dc94e506e69ecb9ec83296eec
页面效果：
![image](http://woojean.com/images/net_8.png)
页面代码：
![image](http://woojean.com/images/net_9.png)


（3）`密码模式`（resource owner password credentials）
用户向客户端提供自己的用户名和密码。客户端使用这些信息，向"服务商提供商"索要授权。这通常用在用户对客户端高度信任的情况下。

（4）`客户端模式`（client credentials）
指客户端以自己的名义，而不是以用户的名义，向"服务提供商"进行认证。严格地说，客户端模式并不属于OAuth框架所要解决的问题。在这种模式中，用户直接向客户端注册，客户端以自己的名义要求"服务提供商"提供服务，其实不存在授权问题。

# IO阻塞、非阻塞、同步、异步
同步和异步
同步和异步是针对应用程序和内核的交互而言的，同步指的是用户进程触发I/O操作并等待或者轮询的去查看I/O操作是否就绪，而异步是指用户进程触发I/O操作以后便开始做自己的事情，而当I/O操作已经完成的时候会得到I/O完成的通知。

阻塞和非阻塞
阻塞和非阻塞是针对于进程在访问数据的时候，根据I/O操作的就绪状态来采取的不同方式，是一种读取或者写入函数的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，读取或者写入函数会立即返回一个状态值。

服务器端有以下几种IO模型：
（1）阻塞式模型（blocking IO）
大部分的socket接口都是阻塞型的（ listen()、accpet()、send()、recv() 等）。阻塞型接口是指系统调用（一般是 IO 接口）不返回调用结果并让当前线程一直阻塞，只有当该系统调用获得结果或者超时出错时才返回。在线程被阻塞期间，线程将无法执行任何运算或响应任何的网络请求，这给多客户机、多业务逻辑的网络编程带来了挑战。
![image](http://woojean.com/images/basic_9.png)
（2）多线程的服务器模型（Multi-Thread）
应对多客户机的网络应用，最简单的解决方式是在服务器端使用多线程（或多进程）。多线程（或多进程）的目的是让每个连接都拥有独立的线程（或进程），这样任何一个连接的阻塞都不会影响其他的连接。但是如果要同时响应成千上万路的连接请求，则无论多线程还是多进程都会严重占据系统资源，降低系统对外界响应效率。
在多线程的基础上，可以考虑使用“线程池”或“连接池”，“线程池”旨在减少创建和销毁线程的频率，其维持一定合理数量的线程，并让空闲的线程重新承担新的执行任务。“连接池”维持连接的缓存池，尽量重用已有的连接、减少创建和关闭连接的频率。这两种技术都可以很好的降低系统开销，都被广泛应用很多大型系统。

（3）非阻塞式模型（Non-blocking IO）
相比于阻塞型接口的显著差异在于，在被调用之后立即返回。
![image](http://woojean.com/images/basic_10.png)
需要应用程序调用许多次来等待操作完成。这可能效率不高，因为在很多情况下，当内核执行这个命令时，应用程序必须要进行`忙碌等待`，直到数据可用为止。
另一个问题，在循环调用非阻塞IO的时候，将大幅度占用CPU，所以一般使用select等来检测”是否可以操作“。

（4）多路复用IO（IO multiplexing）
支持I/O复用的系统调用有select、poll、epoll、kqueue等。使用Select返回后，仍然需要轮询再检测每个socket的状态（读、写），这样的轮训检测在大量连接下也是效率不高的。因为当需要探测的句柄值较大时，select () 接口本身需要消耗大量时间去轮询各个句柄。
很多操作系统提供了更为高效的接口，如 linux 提供 了 epoll，BSD 提供了 kqueue，Solaris 提供了 /dev/poll …。如果需要实现更高效的服务器程序，类似 epoll 这样的接口更被推荐。
![image](http://woojean.com/images/basic_11.png)

（5）使用事件驱动库libevent的服务器模型
libevent是一个事件触发的网络库，适用于windows、linux、bsd等多种平台，内部使用select、epoll、kqueue、IOCP等系统调用管理事件机制。著名分布式缓存软件memcached也是基于libevent，而且libevent在使用上可以做到跨平台。
libevent 库提供一种事件机制，它作为底层网络后端的包装器。`事件系统让为连接添加处理函数变得非常简便，同时降低了底层IO复杂性。这是 libevent 系统的核心`。
创建 libevent 服务器的基本方法是，注册当发生某一操作（比如接受来自客户端的连接）时应该执行的函数，然后调用主事件循环 event_dispatch()。执行过程的控制现在由 libevent 系统处理。注册事件和将调用的函数之后，事件系统开始自治；在应用程序运行时，可以在事件队列中添加（注册）或 删除（取消注册）事件。事件注册非常方便，可以通过它添加新事件以处理新打开的连接，从而构建灵活的网络处理系统。

（6）信号驱动IO模型（Signal-driven IO）
让内核在描述符就绪时发送SIGIO信号通知应用程序。
![image](http://woojean.com/images/basic_12.png)

（7）异步IO模型（asynchronous IO）
告知内核启动某个操作，并`让内核`在整个操作（`包括将数据从内核复制到我们自己的缓冲区`）完成后通知我们。这种模型与信号驱动模型的主要区别在于：信号驱动式I/O是由内核通知我们何时可以启动一个I/O操作，而异步I/O模型是由内核通知我们I/O操作何时完成。
![image](http://woojean.com/images/basic_13.png)

同步和异步IO的区别：
A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;
An asynchronous I/O operation does not cause the requesting process to be blocked; 
两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义阻塞、非阻塞、IO多路复用其实都属于同步IO。

**<font color='red'>非阻塞与异步IO的区别</font>**
在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作（分为两步：准备数据、将数据从内核复制到用户空间）交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。

# redis HyperLogLog

基数：如数据集 {1, 3, 5, 7, 5, 7, 8}，那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。
Redis HyperLogLog是用来做基数统计的算法，HyperLogLog的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。因为HyperLogLog只会根据输入元素来计算基数，而不会储存输入元素本身，所以HyperLogLog不能像集合那样，返回输入的各个元素。
例如：
redis 127.0.0.1:6379> PFADD w3ckey "redis"
1) (integer) 1
redis 127.0.0.1:6379> PFADD w3ckey "mongodb"
1) (integer) 1
redis 127.0.0.1:6379> PFADD w3ckey "mysql"
1) (integer) 1
redis 127.0.0.1:6379> PFCOUNT w3ckey
(integer) 3

基本命令：
（1）PFADD key element [element ...]    # 添加指定元素到HyperLogLog中
（2）PFCOUNT key [key ...]          # 返回给定 HyperLogLog 的基数估算值
（3）PFMERGE destkey sourcekey [sourcekey ...]    # 将多个HyperLogLog合并为一个HyperLogLog

## HTTP协议缓存协商机制相关的6个HTTP头

HTTP缓存协商机制基于6个HTTP头信息进行，动态内容本身并不受浏览器缓存机制的排斥，**只要HTTP头信息中包含相应的缓存协商信息，动态内容一样可以被浏览器缓存**。不过对于POST类型的请求，浏览器一般不启用本地缓存。除了浏览器缓存，HTTP缓存协商机制同样适用于HTTP缓存代理服务器。

主要涉及以下6个HTTP Header：
`Expires`

`Cache-Control`

`Last-Modified`、`If-Modified-Since`

``ETag`、`If-None-Match`。

**Expires/Cache-Control**是控制浏览器**是否直接从浏览器缓存取数据还是重新发请求到服务器取数据**。只是Cache-Control比Expires可以控制的多一些，而且**Cache-Control会重写Expires的规则**。Cache-Control常见的取值有private、no-cache、max-age、must-revalidate等。如果指定Cache-Control的值为private、no-cache、must-revalidate，那么打开新窗口访问时都会重新访问服务器。而如果指定了max-age值，那么**在此值内的时间里就不会重新访问服务器**，例如：`Cache-control: max-age=5`表示当访问此网页后的5秒内再次访问不会去服务器。

**Last-Modified/If-Modified-Since**和**ETag/If-None-Match**是**浏览器发送请求到服务器后判断文件是否已经修改过**，如果没有修改过就只发送一个304回给浏览器，告诉浏览器直接从自己本地的缓存取数据；如果修改过那就整个数据重新发给浏览器。

## Expires和Cache-Control max-age的区别与联系

1. Expires在HTTP/1.0中已经定义，Cache-Control:max-age在HTTP/1.1中才有定义。
2. Expires指定一个**绝对的过期时间**(GMT格式)，这么做会导致至少2个问题：
* 客户端和服务器时间不同步导致Expires的配置出现问题。
* 很容易在配置后忘记具体的过期时间，导致过期来临出现浪涌现象；（而Cache-Control:max-age指定的是从文档被访问后的存活时间，这个时间是个相对值，相对的是文档第一次被请求时服务器记录的请求时间。
3. Expires指定的时间可以是相对文件的最后访问时间或者修改时间，而max-age相对对的是文档的请求时间。
4. 在Apache中，max-age是根据Expires的时间来计算出来的max-age = expires- request_time:(mod_expires.c)

目前主流的浏览器都将HTTP/1.1作为首选，所以当HTTP响应头中同时含有Expires和Cache-Control时，浏览器会优先考虑Cache-Control。


## Last-Modified/If-Modified-Since和ETag/If-None-Match工作方式

1. 浏览器把缓存文件的最后修改时间通过If-Modified-Since来告诉Web服务器（浏览器缓存里存储的不只是网页文件，还有服务器发过来的该文件的最后服务器修改时间）。服务器会把这个时间与服务器上实际文件的最后修改时间进行比较。如果时间一致，那么返回HTTP状态码304（但不返回文件内容），客户端接到之后，就直接把本地缓存文件显示到浏览器中。如果时间不一致，就返回HTTP状态码200和新的文件内容，客户端接到之后，会丢弃旧文件，把新文件缓存起来，并显示到浏览器中（当文件发生改变，或者第一次访问时，服务器返回的HTTP头标签中有Last-Modified，告诉客户端页面的最后修改时间）。

2. 浏览器把缓存文件的ETag，通过If-None-Match，来告诉Web服务器。思路与第一种类似。

**一个例子**
Request Headers
```
Host localhost
User-Agent Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.8.1.16) Gecko/20080702 Firefox/2.0.0.16
Accept text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5
...
If-Modified-Since Tue, 19 Aug 2008 06:49:35GMT
If-None-Match 7936caeeaf6aee6ff8834b381618b513
Cache-Control max-age=0
```

Response Headers
```
Date Tue, 19 Aug 2008 06:50:19 GMT
...
Expires Tue, 19 Aug 2008 07:00:19 GMT
Last-Modified Tue, 19 Aug 2008 06:49:35GMT
Etag 7936caeeaf6aee6ff8834b381618b513
```

对应以上两组缓存控制Header，按F5刷新浏览器和在地址栏里输入网址然后回车。这两个行为是不一样的。**按F5刷新浏览器，浏览器会去Web服务器验证缓存。如果是在地址栏输入网址然后回车，浏览器会直接使用有效的缓存，而不会发http request去服务器验证缓存，这种情况叫做`缓存命中`**。

Cache-Control: public 指可以`公有缓存`，可以是数千名用户共享的。
Cache-Control: private 指只支持`私有缓存`，私有缓存是单个用户专用的。
此外，针对不同的Cache-Control值，对浏览器执行不同的操作，其缓存访问行为也不一样，这些操作包括：打开新窗口、在地址栏回车、按后退按钮、按刷新按钮。

## Last-Modified/If-Modified-Since和ETag/If-None-Match工作方式的区别
```
<?php
  header('Last-Modified:' . gmdate('D, d M Y H:i:s') . ' GMT');
  echo time();
?>
```

此时再通过浏览器请求该动态文件，HTTP响应中将会添加一个头信息：
```
Last-Modified:Fri, 20 Mar 2009 07:53:02 GMT
```

对于带有`Last-Modified`的响应，浏览器会对文件进行缓存，并打上一些标记，下次再发出请求时会带上如下的HTTP头信息：
```
If-Modified-Since:Fri, 20 Mar 2009 07:53:02 GMT
```

如果没有修改，服务器会返回304信息：
```
HTTP/1.1 304 Not Modified
...
```
意味着浏览器可以直接使用本地缓存的内容。

**使用基于最后修改时间的缓存协商存在一些缺点**：
1. 很可能文件内容没有变化，而只是时间被更新，此时浏览器仍然会获取全部内容。
2. 当使用多台机器实现负载均衡时，用户请求会在多台机器之间轮询，而不同机器上的相同文件最后修改时间很难保持一致，可能导致用户的请求每次切换到新的服务器时就需要重新获取所有内容。

比如服务器返回如下带ETag的响应：
```
ETag:"74123-b-938fny4nfi8"
```

浏览器在下次请求该内容时会在HTTP头中添加如下信息：
```
If-None-Match:"74123-b-938fny4nfi8"
```
如果相同的话，服务器返回304。
Web服务器可以自由定义ETag的格式和计算方法。

## GET和POST在TCP层的区别

GET产生一个TCP数据包，POST产生两个TCP数据包：
对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）；
而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。


# 面向对象编程应该遵守的几个原则
## 依赖倒换原则
要针对接口编程，不要对实现编程
1. 高层模块不应该依赖于低层模块。
2. 抽象不应该依赖细节。细节应该依赖抽象。

![image](https://github.com/woojean/woojean.github.io/blob/master/assets/images/dm_1.png)


## 单一职责原则
就一个类而言，应该仅有一个引起它变化的原因。如果一个类承担的职责过多，就等于把这些职责耦合在一起，一个职责的变化可能会削弱或者抑制这个类完成其他职责的能力。这种耦合会导致脆弱的设计，当变化发生时，设计会遭受到意想不到的破坏。比如设计游戏显示区域，将绝对坐标改成相对坐标，实现程序逻辑和界面的分离。


## 开放-封闭原则
软件实体（类、模块、函数等等）应该可以扩展，但是不可修改。
面对需求，对程序的改动是通过增加新代码进行的，而不是更改现有的代码。
最初编写代码时，假设变化不会发生，当变化发生时，就创建抽象来隔离以后发生的同类变化。
开发人员应该仅对程序中呈现出频繁变化的那部分作出抽象，然而对于程序中的每个部分都刻意地进行抽象同样不是一个好主意。


## 迪米特法则
如果两个类不必彼此直接通信，那么这两个类就不应当发生直接的相互作用。如果其中一个类需要调用另一个类的某一个方法的话，可以通过第三者转发这个调用。在类的结构设计上，每一个类都应当尽量降低成员的访问权限。其根本思想是强调了类之间的松耦合，类之间的耦合越弱，越有利于复用，一个处在弱耦合的类被修改，不会对有关系的类造成波及。


## 里氏代换原则
一个软件实体，如果使用的是一个父类的话，那么一定适用于其子类，而且它觉察不出父类对象和子类对象的区别。正是由于子类型的可替换性才使得使用父类类型的模块在无需修改的情况下就可以扩展。

# 秒杀系统优化思路
将请求尽量拦截在系统上游，并且充分利用缓存。由上游至低层优化如下：

## 1.前端（浏览器、APP）

控制实际往后端发送请求的数量，如用户点击“查询”后，将按钮置灰，禁止用户在短时间内重复提交。

## 2.站点层（访问后端数据，拼写html返回）

对uid进行请求计数和去重，比如5秒内只准透过一个请求（可以使用redis设置过期时间实现）。缺点是当有多台机器时（此时相当于5s内限制n个访问），数据可能不准（脏读,但数据库层面真实数据是没问题的）。
假设有海量真实的对站点层的请求，可以通过增加机器来扩容，实在不行只能抛弃部分请求（返回稍后再试），原则是要保护系统，不能让所有用户都失败；

## 3.服务层（提供数据访问）

对于读请求，使用缓存。
对于写请求，使用请求队列（队列成本很低），每次只透有限的写请求（如总票数）去数据层，如果均成功，再放下一批。可以不用统一一个队列，这样的话每个服务透过更少量的请求（总票数/服务个数），这样简单。统一一个队列又复杂了。对于失败的处理无需重放，返回用户查询失败或者下单失败，架构设计原则之一是“fail fast”。

## 4.数据层（数据库、缓存）

经过以上步骤，到数据库层的请求已经有限。

此外还可以做一些业务规则上的优化，如：12306分时分段售票、数据粒度优化（如只展示有、无，而不是具体的数量）、业务逻辑异步（先创建订单，但是状态为未支付，如果超时仍未支付，则恢复库存）。

# 依赖注入、控制反转
IoC（Inversion of Control）控制反转
DI（Dependency Injection）依赖注入

DI是IoC的一种具体实现，另一种主要的实现方式是服务定位器（Service Locator）。

没有IoC的时候，常规的A类使用C类的示意图：
 ![image](http://woojean.com/images/img_1.png)

有IoC的时候，A类不再主动去创建C，而是被动等待，等待IoC的容器获取一个C的实例，然后反向地注入到A类中。
 ![image](http://woojean.com/images/img_2.png)

# cgi.fix_pathinfo配置项的作用是什么？

如果webserver为nginx，则须在PHP的配置文件php.ini中配置cgi.fix_pathinfo = 0，防止nginx文件解析漏洞。
在cgi.fix_pathinfo = 1的情况下，假设有如下的 URL：http://xxx.net/foo.jpg，当访问 http://xxx.net/foo.jpg/a.php 时，foo.jpg 将会被执行，如果 foo.jpg 是一个普通文件，那么 foo.jpg 的内容会被直接显示出来，但是如果把一段 php 代码保存为 foo.jpg，那么问题就来了，这段代码就会被直接执行。

# fastcgi_index配置的作用是什么？

语法：fastcgi_index file 
默认值：none 
使用字段：http, server, location 
如果URI以斜线结尾，文件名将追加到URI后面，这个值将存储在变量$fastcgi_script_name中。
例如：
fastcgi_index  index.php;
fastcgi_param  SCRIPT_FILENAME  /home/www/scripts/php$fastcgi_script_name;
请求"/page.php"的参数SCRIPT_FILENAME将被设置为"/home/www/scripts/php/page.php"，但是请求"/"则为"/home/www/scripts/php/index.php"。

# fastcgi_param的内容是什么？

即为fastcgi模块设置一些服务器环境变量：
fastcgi_param  SCRIPT_FILENAME    $document_root$fastcgi_script_name;			#脚本文件请求的路径
fastcgi_param  QUERY_STRING       $query_string; 					#请求的参数;如?app=123
fastcgi_param  REQUEST_METHOD     $request_method; 					#请求的动作(GET,POST)
fastcgi_param  CONTENT_TYPE       $content_type; 					#请求头中的Content-Type字段
fastcgi_param  CONTENT_LENGTH     $content_length; 					#请求头中的Content-length字段

fastcgi_param  SCRIPT_NAME        $fastcgi_script_name; 			#脚本名称 
fastcgi_param  REQUEST_URI        $request_uri; 					#请求的地址不带参数
fastcgi_param  DOCUMENT_URI       $document_uri; 					#与$uri相同。 
fastcgi_param  DOCUMENT_ROOT      $document_root; #网站的根目录。在server配置中root指令中指定的值 
fastcgi_param  SERVER_PROTOCOL    $server_protocol; 	#请求使用的协议，通常是HTTP/1.0或HTTP/1.1 

fastcgi_param  GATEWAY_INTERFACE  CGI/1.1;							#cgi 版本
fastcgi_param  SERVER_SOFTWARE    nginx/$nginx_version;				#nginx 版本号，可修改、隐藏

fastcgi_param  REMOTE_ADDR        $remote_addr; 					#客户端IP
fastcgi_param  REMOTE_PORT        $remote_port; 					#客户端端口
fastcgi_param  SERVER_ADDR        $server_addr; 					#服务器IP地址
fastcgi_param  SERVER_PORT        $server_port; 					#服务器端口
fastcgi_param  SERVER_NAME        $server_name; 	#服务器名，域名在server配置中指定的server_name

//fastcgi_param  PATH_INFO           $path_info;						#可自定义变量

// PHP only, required if PHP was built with --enable-force-cgi-redirect
// fastcgi_param  REDIRECT_STATUS    200;

在php可打印出上面的服务环境变量：
如：echo $_SERVER['REMOTE_ADDR']

# fastcgi_param配置的作用是什么？

语法：fastcgi_param parameter value 
默认值：none 
使用字段：http, server, location 
指定一些传递到FastCGI服务器的参数。可以使用字符串，变量，或者其组合，这里的设置不会继承到其他的字段，设置在当前字段会清除掉任何之前的定义。
下面是一个PHP需要使用的最少参数：
  	fastcgi_param  SCRIPT_FILENAME  	/home/www/scripts/php$fastcgi_script_name;  
fastcgi_param  QUERY_STRING     	$query_string;
PHP使用SCRIPT_FILENAME参数决定需要执行哪个脚本，QUERY_STRING包含请求中的某些参数。

如果要处理POST请求，则需要另外增加三个参数：
  	fastcgi_param  REQUEST_METHOD   $request_method;  
fastcgi_param  CONTENT_TYPE     $content_type;  
fastcgi_param  CONTENT_LENGTH   $content_length;

如果PHP在编译时带有--enable-force-cgi-redirect，则必须传递值为200的REDIRECT_STATUS参数：
fastcgi_param  REDIRECT_STATUS  200;

# fastcgi_pass配置的作用是什么？

语法：fastcgi_pass fastcgi-server 
默认值：none 
使用字段：http, server, location 
指定FastCGI服务器监听端口与地址。
可以是本机或者其它：
fastcgi_pass   localhost:9000;

使用Unix socket:
fastcgi_pass   unix:/tmp/fastcgi.socket;

同样可以使用一个upstream字段名称：
```
upstream backend  {  
server   localhost:1234;
} 
fastcgi_pass   backend;
```

# fastcgi_read_timeout配置的作用是什么？

语法：fastcgi_read_timeout time 
默认值：fastcgi_read_timeout 60 
使用字段：http, server, location 
前端FastCGI服务器的响应超时时间，如果有一些直到它们运行完才有输出的长时间运行的FastCGI进程，或者在错误日志中出现前端服务器响应超时错误，可能需要调整这个值。

# Nginx支持的IO模型有哪些？

Nginx支持如下处理连接的方法（I/O复用方法），这些方法可以通过use指令指定：
（1）select 如果当前平台没有更有效的方法，它是编译时默认的方法。可以使用配置参数–with-select_module 和 –without-select_module来启用或禁用这个模块。
（2）poll 如果当前平台没有更有效的方法，它是编译时默认的方法。可以使用配置参数–with-poll_module和–without-poll_module来启用或禁用这个模块。
（3）kqueue 高效的方法，使用于FreeBSD 4.1+、 OpenBSD 2.9+、NetBSD 2.0和MacOS X.。使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。
（4）epoll 高效的方法，使用于Linux内核2.6版本及以后的系统。在某些发行版本中，如SuSE 8.2, 有让2.4版本的内核支持epoll的补丁。
（5）rtsig 可执行的实时信号，使用于Linux内核版本2.2.19以后的系统。默认情况下整个系统中不能出现大于1024个POSIX实时(排队)信号。这种情况对于高负载的服务器来说是低效的；所以有必要通过调节内核参数 /proc/sys/kernel/rtsig-max来增加队列的大小。可是从Linux内核版本2.6.6-mm2开始， 这个参数就不再使用了，并且对于每个进程有一个独立的信号队列，这个队列的大小可以用 RLIMIT_SIGPENDING 参数调节。当这个队列过于拥塞，nginx就放弃它并且开始使用poll方法来处理连接直到恢复正常。
（6）/dev/poll 高效的方法，使用于 Solaris 7 11/99+, HP/UX 11.22+ (eventport), IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+.
（7）eventport 高效的方法，使用于 Solaris 10。
在linux下面，只有epoll是高效的方法。

# Nginx有哪些内置的全局变量？

$args						请求中的参数;
$content_length				HTTP请求信息里的"Content-Length";
$content_type				请求信息里的"Content-Type";
$document_root				针对当前请求的根路径设置值;
$document_uri				与$uri相同;
$host						http请求的域名
$http_user_agent			客户端agent信息;
$http_cookie				客户端cookie信息;
$limit_rate					对连接速率的限制;
$request_body_file			客户端请求主体信息的临时文件名;
$request_method				请求的方法，比如"GET"、"POST"等;
$remote_addr				客户端地址;
$remote_port				客户端端口号;
$remote_user				客户端用户名，认证用;
$request_filename			当前请求的文件路径名;
$request_body_file			客户端请求主体的临时文件名;
$request_uri				包含请求参数的原始URI，不包含主机名，如："/foo/bar.php?arg=baz";
$query_string				与$args相同;
$scheme						所用的协议，比如http或者是https;
$server_addr				服务器地址，如果没有用listen指明服务器地址，使用这个变量将发起一次系统调用以取得地址(造成资源浪费);
$server_name				请求到达的服务器名;
$server_port				请求到达的服务器端口号;
$uri						不带请求参数的当前URI，$uri不包含主机名，如"/foo/bar.html";

$fastcgi_script_name		这个变量等于一个以斜线结尾的请求URI加上fastcgi_index给定的参数。可以用这个变量代替SCRIPT_FILENAME 和PATH_TRANSLATED，以确定php脚本的名称。
如请求"/info/": 
 	fastcgi_index		index.php;  
fastcgi_param  		SCRIPT_FILENAME  	/home/www/scripts/php$fastcgi_script_name;
SCRIPT_FILENAME等于"/home/www/scripts/php/info/index.php"

# Nginx的模块及工作原理是怎样的？是如何有FastCGI配合的？

Nginx由内核和模块组成，内核的设计非常简洁，仅仅通过查找配置文件将客户端请求映射到一个location block（location是Nginx配置中的一个指令，用于URL匹配），而在这个location中所配置的每个指令将会启动不同的模块去完成相应的工作。

Nginx的模块直接被编译进Nginx，因此属于静态编译方式。启动Nginx后，Nginx的模块被自动加载，不像Apache首先将模块编译为一个so文件，然后在配置文件中指定是否进行加载。在解析配置文件时，Nginx的每个模块都有可能去处理某个请求，但是同一个处理请求只能由一个模块来完成。 

在工作方式上，Nginx分为单工作进程和多工作进程两种模式。在单工作进程模式下，除主进程外，还有一个工作进程，工作进程是单线程的；在多工作进程模式下，每个工作进程包含多个线程。Nginx默认为单工作进程模式。

Nginx不支持对外部程序的直接调用或者解析，所有的外部程序（包括PHP）必须通过FastCGI接口来调用。FastCGI接口在Linux下是socket（这个socket可以是文件socket，也可以是ip socket）。

为了调用CGI程序，还需要一个FastCGI的wrapper（wrapper可以理解为用于启动另一个程序的程序），这个wrapper绑定在某个固定socket上，如端口或者文件socket。当Nginx将CGI请求发送给这个socket的时候，通过FastCGI接口，wrapper接收到请求，然后Fork(派生）出一个新的线程，这个线程调用解释器或者外部程序处理脚本并读取返回数据；接着，wrapper再将返回的数据通过FastCGI接口，沿着固定的socket传递给Nginx；最后，Nginx将返回的数据（html页面或者图片）发送给客户端。这就是Nginx+FastCGI的整个运作过程

FastCGI接口方式在脚本解析服务器上启动一个或者多个守护进程对动态脚本进行解析，这些进程就是FastCGI进程管理器，或者称为FastCGI引擎，如PHP-FPM。因此HTTPServer完全解放出来，可以更好地进行响应和并发处理。
其实，Nginx就是一个反向代理服务器。Nginx通过反向代理功能将动态请求转向后端php-fpm（wrapper），从而实现对PHP的解析支持，这就是Nginx实现PHP动态解析的原理。

其整体工作流程：
1) FastCGI进程管理器php-fpm自身初始化，启动主进程php-fpm和启动start_servers个CGI 子进程。
主进程php-fpm主要是管理fastcgi子进程，监听9000端口。
fastcgi子进程等待来自Web Server的连接。

当客户端请求到达Web Server Nginx时，Nginx通过location指令，将所有以php为后缀的文件都交给127.0.0.1:9000来处理，即Nginx通过location指令，将所有以php为后缀的文件都交给127.0.0.1:9000来处理。

FastCGI进程管理器PHP-FPM选择并连接到一个子进程CGI解释器。Web server将CGI环境变量和标准输入发送到FastCGI子进程。

FastCGI子进程完成处理后将标准输出和错误信息从同一连接返回Web Server。当FastCGI子进程关闭连接时，请求便告处理完成。

5) FastCGI子进程接着等待并处理来自FastCGI进程管理器（运行在 WebServer中）的下一个连接。

# Nginx的配置

nginx配置文件主要分为六个区域：
main
控制子进程的所属用户/用户组、派生子进程数、错误日志位置/级别、pid位置、子进程优先级、进程对应cpu、进程能够打开的文件描述符数目等
events
控制nginx处理连接的方式
（3）http
（4）sever
（5）location
（6）upstream

实例：
```
user www-data;    								# 运行用户
worker_processes  1;							# 启动进程数,通常设置成和cpu的数量相等
error_log  /var/log/nginx/error.log;			# 全局错误日志
pid        /var/run/nginx.pid;					# PID文件

// 工作模式及连接数上限
events {
use   epoll;             					# 使用epoll多路复用模式
worker_connections  1024;					# 单个后台worker process进程的最大并发链接数
    # multi_accept on; 
}

// 设定http服务器，利用它的反向代理功能提供负载均衡支持
http {
    # 设定mime类型,类型由mime.type文件定义
    include       /etc/nginx/mime.types;
default_type  application/octet-stream; 	# 1 octet = 8 bit

    # 设定访问日志
    access_log    /var/log/nginx/access.log;

    # sendfile指令指定nginx是否调用sendfile函数（zero copy方式）来输出文件，对于普通应用，必须设为on,如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的uptime.
    sendfile        on;
    #tcp_nopush     on;					# 在一个数据包里发送所有头文件，而不一个接一个的发送

    
    keepalive_timeout  65;				# 连接超时时间
    tcp_nodelay        on;				# 作用于socket参数TCP_NODELAY，禁用nagle算法，也即不缓存数据
    
    # 开启gzip压缩
    gzip  on;
    gzip_disable "MSIE [1-6]\.(?!.*SV1)";

    # 设定请求缓冲
    client_header_buffer_size    1k;
    large_client_header_buffers  44k;

    include /etc/nginx/conf.d/*.conf;
    include /etc/nginx/sites-enabled/*;

    # 设定负载均衡的服务器列表
    upstream mysvr {
    	# weigth参数表示权值，权值越高被分配到的几率越大
    	# 本机上的Squid开启3128端口
    	server 192.168.8.1:3128 	weight=5;
    	server 192.168.8.2:80  		weight=1;
    	server 192.168.8.3:80  		weight=6;
    }

   server {
        listen	80;						# 侦听80端口
        server_name  www.xx.com;		# 定义使用www.xx.com访问
        access_log  logs/www.xx.com.access.log  main;		# 设定本虚拟主机的访问日志

    # 默认请求
    location / {
		root   /root;      						# 定义服务器的默认网站根目录位置
		index index.php index.html index.htm;   # 定义首页索引文件的名称
fastcgi_pass  localhost:9000;				
    	fastcgi_param  SCRIPT_FILENAME  $document_root/$fastcgi_script_name; 
		include /etc/nginx/fastcgi_params;
	}

    # 定义错误提示页面
    error_page   500 502 503 504 /50x.html;  
        location = /50x.html {
        root   /root;
    }

    # 静态文件，nginx自己处理
    location ~ ^/(images|javascript|js|css|flash|media|static)/ {
        root /var/www/virtual/htdocs;
        # 过期时间30天
        expires 30d;
}

    # PHP脚本请求全部转发到FastCGI处理，使用FastCGI默认配置
    location ~ \.php$ {
        root /root;
        fastcgi_pass 127.0.0.1:9000;
        fastcgi_index index.php;
        fastcgi_param SCRIPT_FILENAME /home/www/www$fastcgi_script_name;
        include fastcgi_params;
}

    # 设定查看Nginx状态的地址
    location /NginxStatus {
        stub_status 			on;
        access_log              on;
        auth_basic              "NginxStatus";
        auth_basic_user_file  	conf/htpasswd;
}

    # 禁止访问 .htxxx 文件
    location ~ /\.ht {
        deny all;
    }
    }
}
```

# 为什么Nginx反向代理能够提高性能？

对于后端是动态服务来说，比如Java和PHP。这类服务器（如JBoss和PHP-FPM）的IO处理能力往往不高。Nginx有个好处是它会把Request在读取完整之前buffer住，这样交给后端的就是一个完整的HTTP请求，从而提高后端的效率，而不是断断续续的传递（互联网上连接速度一般比较慢）。同样，Nginx也可以把response给buffer住，同样也是减轻后端的压力。

# 什么是Nginx重写？与Location有什么区别？Location的匹配规则是什么样的？

rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在server{},location{},if{}中，并且只能对域名后边的除去传递的参数外的字符串起作用。如，http://seanlook.com/a/we/index.php?id=1&u=str 只对/a/we/index.php重写。
如果想对域名或参数字符串起作用，可以使用全局变量匹配，也可以使用proxy_pass反向代理。

Rewrite标志位：
last 			相当于Apache的[L]标记，表示完成rewrite
break			停止执行当前虚拟主机的后续rewrite指令集
redirect		返回302临时重定向，地址栏会显示跳转后的地址
permanent		返回301永久重定向，地址栏会显示跳转后的地址

Rewrite实例：
```
// 应用于Server
server {
listen 80;
server_name start.igrow.cn;
index index.html index.php;
root html;
if ($http_host !~ “^star\.igrow\.cn$&quot {
rewrite ^(.*) http://star.igrow.cn$1 redirect;
}
}

// 防盗链
location ~* \.(gif|jpg|swf)$ {
valid_referers none blocked start.igrow.cn sta.igrow.cn;
if ($invalid_referer) {
rewrite ^/ http://$host/logo.png;
}
}

// 根据文件类型设置过期时间
location ~* \.(js|css|jpg|jpeg|gif|png|swf)$ {
if (-f $request_filename) {
expires 1h;
break;
}
}

// 禁止访问某个目录
location ~* \.(txt|doc)${
root /data/www/wwwroot/linuxtone/test;
deny all;
}
```
rewrite和location：
rewrite和location都能实现跳转，主要区别在于rewrite是在同一域名内更改获取资源的路径，而location是对一类路径做控制访问或反向代理，可以proxy_pass到其他机器。很多情况下rewrite也会写在location里，它们的执行顺序是：
（1）执行server块的rewrite指令
（2）执行location匹配
（3）执行选定的location中的rewrite指令
如果其中某步URI被重写，则重新循环执行1-3，直到找到真实存在的文件；循环超过10次，则返回500 Internal Server Error错误。

正则匹配会覆盖普通匹配，location的执行逻辑跟location的编辑顺序无关。
语法格式：location [=|~|~*|^~|@] /uri/ { … } 

=		表示精确匹配
~ 		区分大小写匹配
~* 		不区分大小写匹配
!~		区分大小写不匹配
!~* 	不区分大小写不匹配
^ 		以什么开头的匹配
$ 		以什么结尾的匹配
^~ 		表示uri以某个常规字符串开头，不是正则匹配，优先级高于正则
/ 		通用匹配,如果没有其它匹配,任何请求都会匹配到
*    代表任意字符

 . 	匹配除换行符以外的任意字符
 ?	重复0次或1次
 +  重复1次或更多次
  *重复0次或更多次
  \d匹配数字
  {n}重复n次
  {n,}重复n次或更多次
  [c]匹配单个字符c
  [a-z]匹配a-z小写字母的任意一个
  \转义字符

  -f和!-f判断是否存在文件
  -d和!-d判断是否存在目录
  -e和!-e判断是否存在文件或目录
  -x和!-x判断文件是否可执行


例：
实际使用中一般至少有三个匹配规则定义，如下：
/* 
直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。
 这里是直接转发给后端应用服务器了，也可以是一个静态首页
 第一个必选规则
*/
location = / {
    proxy_pass http://tomcat:8080/index
}

/*
 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项
 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用
*/
location ^~ /static/ {
    root /webroot/static/;
}
location ~* \.(gif|jpg|jpeg|png|css|js|ico)$ {
    root /webroot/res/;
}

/*
 第三个规则就是通用规则，用来转发动态请求到后端应用服务器
 非静态文件请求就默认是动态请求，自己根据实际把握
 毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了
*/
location / {
    proxy_pass http://tomcat:8080/
}

# 如何使用Nginx实现负载均衡和反向代理？

设定http服务器，利用它的反向代理功能提供负载均衡支持

```
http {
     #设定mime类型,类型由mime.type文件定义
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    #设定日志格式
    access_log    /var/log/nginx/access.log;

    #其他配置，略

    #设定负载均衡的服务器列表
    upstream mysvr {
    	#weigth参数表示权值，权值越高被分配到的几率越大
    	server 192.168.8.1x:3128 weight=5;#本机上的Squid开启3128端口
    	server 192.168.8.2x:80  weight=1;
    	server 192.168.8.3x:80  weight=6;
    }

   	upstream mysvr2 {
    	#weigth参数表示权值，权值越高被分配到的几率越大
    	server 192.168.8.x:80  weight=1;
    	server 192.168.8.x:80  weight=6;
    }

   #第一个虚拟服务器
   server {
    	#侦听192.168.8.x的80端口
        listen       80;
        server_name  192.168.8.x;

      	#对aspx后缀的进行负载均衡请求
    	location ~ .*\.aspx$ {
         	root   /root;      						#定义服务器的默认网站根目录位置
          	index index.php index.html index.htm;   #定义首页索引文件的名称
          	proxy_pass  http://mysvr ;				#请求转向mysvr定义的服务器列表

          	# 反向代理的配置
          	proxy_redirect off;

          	#后端的Web服务器可以通过X-Forwarded-For获取用户真实IP
          	proxy_set_header Host $host;
          	proxy_set_header X-Real-IP $remote_addr;
          	proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          	client_max_body_size 10m;    		#允许客户端请求的最大单文件字节数
          	client_body_buffer_size 128k;  		#缓冲区代理缓冲用户端请求的最大字节数
          	proxy_connect_timeout 90;  			#nginx跟后端服务器连接超时时间(代理连接超时)
          	proxy_send_timeout 90;        		#后端服务器数据回传时间(代理发送超时)
          	proxy_read_timeout 90;         		#连接成功后，后端服务器响应时间(代理接收超时)
          	proxy_buffer_size 4k;             #设置代理服务器（nginx）保存用户头信息的缓冲区大小
          	proxy_buffers 4 32k;         #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置
          	proxy_busy_buffers_size 64k;    #高负荷下缓冲大小（proxy_buffers*2）
          	proxy_temp_file_write_size 64k;  #设定缓存文件夹大小，大于这个值，将从upstream服务器传
       }
	}
}
```

# 安装Nginx依赖哪些条件？

（1）编译环境gcc g++ 开发库之类的需要提前装好
（2）安装PCRE库，为了重写（rewrite）：PCRE(Perl Compatible Regular Expressions)是一个Perl库，包括perl兼容的正则表达式库。
（3）安装zlib库，为了gzip压缩。
（4）安装ssl

./configure --sbin-path=/usr/local/nginx/nginx 
--conf-path=/usr/local/nginx/nginx.conf 
--pid-path=/usr/local/nginx/nginx.pid 
--with-http_ssl_module
--with-pcre=/usr/local/src/pcre-8.34 			
--with-zlib=/usr/local/src/zlib-1.2.8 			
--with-openssl=/usr/local/src/openssl-1.0.1c

# 服务器出现大量TIME_WAIT和CLOSE_WAIT的可能原因是什么？

查看当前服务器网络连接状态：
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
输出：
TIME_WAIT 814
CLOSE_WAIT 1
FIN_WAIT1 1
ESTABLISHED 634
SYN_RECV 2
LAST_ACK 1

常用的三个状态是：ESTABLISHED 表示正在通信，TIME_WAIT 表示主动关闭，CLOSE_WAIT 表示被动关闭。
![image](http://woojean.com/images/net_12.png)

如果服务器出了异常，百分之八九十都是下面两种情况：
1.服务器保持了大量TIME_WAIT状态
2.服务器保持了大量CLOSE_WAIT状态
因为linux分配给一个用户的文件句柄是有限的，而TIME_WAIT和CLOSE_WAIT两种状态如果一直被保持，那么意味着对应数目的通道就一直被占着，一旦达到句柄数上限，新的请求就无法被处理了，接着就是大量Too Many Open Files异常。

服务器保持了大量TIME_WAIT状态的原因：
TIME_WAIT是主动关闭连接的一方保持的状态，对于爬虫服务器来说他本身就是“客户端”，在完成一个爬取任务之后，他就 会发起主动关闭连接，从而进入TIME_WAIT的状态，然后在保持这个状态2MSL（max segment lifetime）时间之后，彻底关闭回收资源。
而对于HTTP的交互跟上面画的那个图是不一样的，`关闭连接的不是客户端，而是服务器`，所以web服务器也是会出现大量的TIME_WAIT的情况的。解决思路很简单，就是让服务器能够快速回收和重用那些TIME_WAIT的资源，通过修改`/etc/sysctl.conf`文件实现。
net.ipv4.tcp_tw_reuse = 1   # 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭  
net.ipv4.tcp_tw_recycle = 1  # 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭  

服务器保持了大量CLOSE_WAIT状态的原因：
TIME_WAIT状态可以通过优化服务器参数得到解决，因为发生TIME_WAIT的情况是服务器自己可控的，要么就是对方连接的异常，要么就是自己没有迅速回收资源，总之不是由于自己程序错误导致的。
如果一直保持在CLOSE_WAIT状态，那么只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出ack信号。换句话说，就是在对方连接关闭之后，程序里没有检测到，或者程序压根就忘记了这个时候需要关闭连接，于是这个资源就一直被程序占着。
如：服 务器A是一台爬虫服务器，它使用简单的HttpClient去请求资源服务器B上面的apache获取文件资源，正常情况下，如果请求成功，那么在抓取完资源后，服务器A会主动发出关闭连接的请求，这个时候就是主动关闭连接，服务器A的连接状态我们可以看到是TIME_WAIT。如果一旦发生异常呢？假设 请求的资源服务器B上并不存在，那么这个时候就会由服务器B发出关闭连接的请求，服务器A就是被动的关闭了连接，如果服务器A被动关闭连接之后程序员忘了 让HttpClient释放连接，那就会造成CLOSE_WAIT的状态了。

## 可以利用incr命令的原子性来实现锁

```c
$value = $redis->get($lock); 
if($value < 1 ){
  $redis->incr($lock,1);
  // ...
  $redis->decr($lock,1);
}
```

不使用incr：
```c
// 被WATCH的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在EXEC执行之前被修改了，那么整个事务都会被取消
WATCH mykey
  $val = GET mykey   // 乐观锁
  $val = $val + 1
MULTI
  SET mykey $val
EXEC
```

## SETNX，是「SET if Not eXists」的缩写，也就是只有不存在的时候才设置。

```c
// 缓存过期时通过SetNX获取锁，如果成功了就更新缓存，然后删除锁
$ok = $redis->setNX($key, $value);
if ($ok) {
    $cache->update();
    $redis->del($key);
}
```
存在问题：如果请求执行因为某些原因意外退出了，导致创建了锁但是没有删除锁，那么这个锁将一直存在，以至于以后缓存再也得不到更新。

因此需要给锁加一个过期时间以防不测。
```php
// 加锁
$redis->multi();
$redis->setNX($key, $value);
$redis->expire($key, $ttl);
$redis->exec();
```
存在问题：当多个请求到达时，虽然只有一个请求的SetNX可以成功，但是任何一个请求的Expire却都可以成功，如此就意味着即便获取不到锁，也可以刷新过期时间，如果请求比较密集的话，那么过期时间会一直被刷新，导致锁一直有效。

从 2.6.12 起，SET涵盖了SETEX的功能，并且SET本身已经包含了设置过期时间的功能：
```php
$ok = $redis->set($key, $value, array('nx', 'ex' => $ttl));
if ($ok) {
    $cache->update();
    $redis->del($key);
}
```
